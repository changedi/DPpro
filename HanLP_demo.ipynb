{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changedi/DPpro/blob/master/HanLP_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owhcMbiD0aid"
      },
      "source": [
        "# use hanlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w27L8dOE0XbV"
      },
      "outputs": [],
      "source": [
        "!pip install hanlp[full] -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vcg76Csf1AhT"
      },
      "source": [
        "## 文本相似\n",
        "two text similarity using pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDDSgOBw0wuT"
      },
      "outputs": [],
      "source": [
        "import hanlp\n",
        "\n",
        "sim = hanlp.load(hanlp.pretrained.sts.STS_ELECTRA_BASE_ZH)\n",
        "print(sim([\n",
        "    ['看图猜一电影名', '看图猜电影'],\n",
        "    ['无线路由器怎么无线上网', '无线上网卡和无线路由器怎么用'],\n",
        "    ['北京到上海的动车票', '上海到北京的动车票'],\n",
        "]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-W_m9RiBgfy"
      },
      "outputs": [],
      "source": [
        "# use fasttext\n",
        "import hanlp\n",
        "import torch\n",
        "\n",
        "# fasttext is a `torch.nn.Module`. Unless you know how to code in\n",
        "# PyTorch, otherwise don't bother to use this.\n",
        "fasttext = hanlp.load(hanlp.pretrained.fasttext.FASTTEXT_WIKI_300_ZH)\n",
        "\n",
        "vec = fasttext('单词')\n",
        "print(vec)\n",
        "\n",
        "print(torch.nn.functional.cosine_similarity(fasttext('单词'), fasttext('词语'), dim=0))\n",
        "print(torch.nn.functional.cosine_similarity(fasttext('单词'), fasttext('今天'), dim=0))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use word2vec\n",
        "import hanlp\n",
        "import torch\n",
        "\n",
        "word2vec = hanlp.load(hanlp.pretrained.word2vec.CONVSEG_W2V_NEWS_TENSITE_WORD_PKU)\n",
        "vec = word2vec('先进')\n",
        "print(vec)\n",
        "\n",
        "print(torch.nn.functional.cosine_similarity(word2vec('先进'), word2vec('优秀'), dim=0))\n",
        "print(torch.nn.functional.cosine_similarity(word2vec('先进'), word2vec('水果'), dim=0))\n",
        "\n",
        "print('获取语义最相似的词语：')\n",
        "print(word2vec.most_similar('上海'))\n",
        "# print(word2vec.most_similar(['上海', '寒冷'])) # batching更快\n",
        "\n",
        "print('非常寒冷是OOV所以无法获取：')\n",
        "print(word2vec.most_similar('非常寒冷'))\n",
        "print('但是在doc2vec模式下OOV也可以进行相似度计算：')\n",
        "print(word2vec.most_similar('非常寒冷', doc2vec=True))\n",
        "print('甚至可以处理短文本：')\n",
        "print(word2vec.most_similar('国家图书馆推出2022年春节主题活动', doc2vec=True))"
      ],
      "metadata": {
        "id": "h3NTMHIB5JU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyrWBAQV20hS"
      },
      "source": [
        "## 分词\n",
        "tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZVVJ2MS2EgL"
      },
      "outputs": [],
      "source": [
        "import hanlp\n",
        "\n",
        "tokenizer = hanlp.load(hanlp.pretrained.tok.LARGE_ALBERT_BASE)\n",
        "print(tokenizer('商品和服务'))\n",
        "print(tokenizer(['萨哈夫说，伊拉克将同联合国销毁伊拉克大规模杀伤性武器特别委员会继续保持合作。',\n",
        "                 '上海华安工业（集团）公司董事长谭旭光和秘书张晚霞来到美国纽约现代艺术博物馆参观。',\n",
        "                 'HanLP支援臺灣正體、香港繁體，具有新詞辨識能力的中文斷詞系統']))\n",
        "\n",
        "text = 'NLP统计模型没有加规则，聪明人知道自己加。英文、数字、自定义词典统统都是规则。'\n",
        "print(tokenizer(text))\n",
        "\n",
        "dic = {'自定义词典': 'custom_dict', '聪明人': 'smart'}\n",
        "\n",
        "\n",
        "def split_by_dic(text: str):\n",
        "    # We use regular expression for the sake of simplicity.\n",
        "    # However, you should use some trie trees for production\n",
        "    import re\n",
        "    p = re.compile('(' + '|'.join(dic.keys()) + ')')\n",
        "    sents, offset, words = [], 0, []\n",
        "    for m in p.finditer(text):\n",
        "        if offset < m.start():\n",
        "            sents.append(text[offset: m.start()])\n",
        "            words.append((m.group(), dic[m.group()]))\n",
        "            offset = m.end()\n",
        "    if offset < len(text):\n",
        "        sents.append(text[offset:])\n",
        "        words.append((None, None))\n",
        "    flat = []\n",
        "    for pred, (word, tag) in zip(tokenizer(sents), words):\n",
        "        flat.extend(pred)\n",
        "        if word:\n",
        "            flat.append((word, tag))\n",
        "    return flat\n",
        "\n",
        "\n",
        "print(split_by_dic(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZcQugu83ZYf"
      },
      "outputs": [],
      "source": [
        "# use trie\n",
        "from hanlp_trie.trie import Trie\n",
        "\n",
        "import hanlp\n",
        "\n",
        "tokenizer = hanlp.load('LARGE_ALBERT_BASE')\n",
        "text = 'NLP统计模型没有加规则，聪明人知道自己加。英文、数字、自定义词典统统都是规则。'\n",
        "print(tokenizer(text))\n",
        "\n",
        "trie = Trie()\n",
        "trie.update({'自定义词典': 'custom_dict', '聪明人': 'smart'})\n",
        "\n",
        "\n",
        "def split_sents(text: str, trie: Trie):\n",
        "    words = trie.parse_longest(text)\n",
        "    sents = []\n",
        "    pre_start = 0\n",
        "    offsets = []\n",
        "    for start, end, value in words:\n",
        "        if pre_start != start:\n",
        "            sents.append(text[pre_start: start])\n",
        "            offsets.append(pre_start)\n",
        "        pre_start = end\n",
        "    if pre_start != len(text):\n",
        "        sents.append(text[pre_start:])\n",
        "        offsets.append(pre_start)\n",
        "    return sents, offsets, words\n",
        "\n",
        "\n",
        "print(split_sents(text, trie))\n",
        "\n",
        "\n",
        "def merge_parts(parts, offsets, words):\n",
        "    items = [(i, p) for (i, p) in zip(offsets, parts)]\n",
        "    items += [(start, [value]) for (start, end, value) in words]\n",
        "    return [each for x in sorted(items) for each in x[1]]\n",
        "\n",
        "\n",
        "tokenizer = hanlp.pipeline() \\\n",
        "    .append(split_sents, output_key=('parts', 'offsets', 'words'), trie=trie) \\\n",
        "    .append(tokenizer, input_key='parts', output_key='tokens') \\\n",
        "    .append(merge_parts, input_key=('tokens', 'offsets', 'words'), output_key='merged')\n",
        "\n",
        "print(tokenizer(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw9a8EHvogp9"
      },
      "source": [
        "## pos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGl5blOSA2Ll"
      },
      "outputs": [],
      "source": [
        "import hanlp\n",
        "from hanlp.pretrained.pos import CTB9_POS_ALBERT_BASE\n",
        "\n",
        "tagger = hanlp.load(CTB9_POS_ALBERT_BASE)\n",
        "print(tagger.predict(['我', '的', '希望', '是', '希望', '世界', '和平']))\n",
        "print(tagger.predict([['支持', '批处理', '地', '预测'], ['速度', '更', '快']]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## use pipeline"
      ],
      "metadata": {
        "id": "c4-Nv2x0E5u3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hanlp\n",
        "\n",
        "tokenizer = hanlp.load('LARGE_ALBERT_BASE')\n",
        "tagger = hanlp.load('CTB9_POS_ALBERT_BASE')\n",
        "syntactic_parser = hanlp.load('CTB7_BIAFFINE_DEP_ZH')\n",
        "semantic_parser = hanlp.load('SEMEVAL16_TEXT_BIAFFINE_ZH')\n",
        "\n",
        "pipeline = hanlp.pipeline() \\\n",
        "    .append(hanlp.utils.rules.split_sentence, output_key='sentences') \\\n",
        "    .append(tokenizer, output_key='tokens') \\\n",
        "    .append(tagger, output_key='part_of_speech_tags') \\\n",
        "    .append(syntactic_parser, input_key=('tokens', 'part_of_speech_tags'), output_key='syntactic_dependencies', conll=False) \\\n",
        "    .append(semantic_parser, input_key=('tokens', 'part_of_speech_tags'), output_key='semantic_dependencies', conll=False)\n",
        "print(pipeline)\n",
        "\n",
        "text = '''HanLP是一系列模型与算法组成的自然语言处理工具包，目标是普及自然语言处理在生产环境中的应用。\n",
        "HanLP具备功能完善、性能高效、架构清晰、语料时新、可自定义的特点。\n",
        "内部算法经过工业界和学术界考验，配套书籍《自然语言处理入门》已经出版。\n",
        "'''\n",
        "\n",
        "doc = pipeline(text)\n",
        "print(doc)\n",
        "# By default the doc is json serializable, it holds true if your pipes output json serializable object too.\n",
        "# print(json.dumps(doc, ensure_ascii=False, indent=2))\n",
        "\n",
        "# You can save the config to disk for deploying or sharing.\n",
        "pipeline.save('zh.json')\n",
        "# Then load it smoothly.\n",
        "deployed = hanlp.load('zh.json')\n",
        "print(deployed)\n",
        "print(deployed(text))"
      ],
      "metadata": {
        "id": "C1mwvOtGByYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AMR(Abstract meaning representation)"
      ],
      "metadata": {
        "id": "Fxj3yPYQ3l7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hanlp\n",
        "\n",
        "parser = hanlp.load(hanlp.pretrained.amr.MRP2020_AMR_ENG_ZHO_XLM_BASE)\n",
        "\n",
        "# For Chinese:\n",
        "print(parser([\"男孩\", \"希望\", \"女孩\", \"相信\", \"他\", \"。\"]))\n",
        "print(parser([\"男孩\", \"希望\", \"女孩\", \"相信\", \"他\", \"。\"], output_amr=False))\n",
        "\n",
        "# For English:\n",
        "print(parser(['The', 'boy', 'wants', 'the', 'girl', 'to', 'believe', 'him', '.'], language='eng'))\n",
        "# It's suggested to also feed the lemma for stabler performance.\n",
        "print(parser([('The', 'the'), ('boy', 'boy'), ('wants', 'want'), ('the', 'the'), ('girl', 'girl'), ('to', 'to'),\n",
        "              ('believe', 'believe'), ('him', 'he'), ('.', '.')], language='eng'))"
      ],
      "metadata": {
        "id": "_oSxoeId3noC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLM(masked language model)"
      ],
      "metadata": {
        "id": "wJNe8vl557cj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hanlp.components.lm.mlm import MaskedLanguageModel\n",
        "\n",
        "mlm = MaskedLanguageModel()\n",
        "mlm.load('bert-base-chinese')\n",
        "print(mlm('生活的真谛是[MASK]。'))\n",
        "\n",
        "# Batching is always faster\n",
        "print(mlm(['生活的真谛是[MASK]。', '巴黎是[MASK][MASK]的首都。']))"
      ],
      "metadata": {
        "id": "7pxkpDXp592T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NER"
      ],
      "metadata": {
        "id": "jH9Yqvni6eTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hanlp\n",
        "from hanlp.components.mtl.tasks.ner.tag_ner import TaggingNamedEntityRecognition\n",
        "from hanlp.utils.io_util import get_resource\n",
        "\n",
        "HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ERNIE_GRAM_ZH)\n",
        "ner: TaggingNamedEntityRecognition = HanLP['ner/msra']\n",
        "ner.dict_whitelist = {'午饭后': 'TIME'}\n",
        "doc = HanLP('2021年测试高血压是138，时间是午饭后2点45，低血压是44', tasks='ner/msra')\n",
        "doc.pretty_print()\n",
        "print(doc['ner/msra'])\n",
        "\n",
        "ner.dict_tags = {('名字', '叫', '金华'): ('O', 'O', 'S-PERSON')}\n",
        "HanLP('他在浙江金华出生，他的名字叫金华。', tasks='ner/msra').pretty_print()\n",
        "\n",
        "# HanLP.save(get_resource(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ERNIE_GRAM_ZH))\n",
        "\n",
        "# 需要算法基础才能理解，初学者可参考 http://nlp.hankcs.com/book.php\n",
        "# See https://hanlp.hankcs.com/docs/api/hanlp/components/mtl/tasks/ner/tag_ner.html"
      ],
      "metadata": {
        "id": "W__K5IuQ6gip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MTL"
      ],
      "metadata": {
        "id": "vqjGMFBA7ie_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hanlp\n",
        "from hanlp_common.document import Document\n",
        "\n",
        "# CLOSE是自然语义标注的闭源语料库，BASE是中号模型，ZH中文\n",
        "HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH)\n",
        "# 默认执行全部任务\n",
        "doc: Document = HanLP(['2021年HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。', '阿婆主来到北京立方庭参观自然语义科技公司。'])\n",
        "# 返回类型Document是dict的子类，打印出来兼容JSON\n",
        "print(doc)\n",
        "# 即时可视化，防止换行请最大化窗口，推荐在Jupyter Notebook里调用\n",
        "doc.pretty_print()\n",
        "# 指定可视化OntoNotes标准的NER\n",
        "# doc.pretty_print(ner='ner/ontonotes', pos='pku')"
      ],
      "metadata": {
        "id": "EQwDJLcA7kEw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1OCTcX9vrijSas07b3OfD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}