{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEvawVkyi114kQBkVwEw5R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changedi/DPpro/blob/master/langchainANDopenaiV1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2Y4S3eBmyBm"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade openai==0.28.1\n",
        "!pip install --upgrade langchain==0.0.331rc1\n",
        "!pip install --upgrade langchain-experimental\n",
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema.messages import HumanMessage, SystemMessage"
      ],
      "metadata": {
        "id": "JemCw2X6m9Rk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "afsJq-in5Zov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import openai\n",
        "# record the time before the request is sent\n",
        "start_time = time.time()\n",
        "\n",
        "# send a ChatCompletion request to count to 100\n",
        "response = openai.ChatCompletion.create(\n",
        "    model='gpt-4-1106-preview',\n",
        "    messages=[\n",
        "        { 'role': 'user',\n",
        "         'content': '请阅读下面这段文章，并回答哈雷彗星下次回归距离现在还有几年？文章如下：哈雷彗星是唯一能用裸眼直接从地球看见的短周期彗星，也是人一生中唯一以裸眼可能看见两次的彗星。其它能以裸眼看见的彗星可能会更壮观和更美丽，但那些都是数千年才会出现一次的彗星。哈雷彗星上一次回归是在1986年，而下一次回归将在2030年中。在1986年回归时，哈雷彗星成为第一颗被宇宙飞船详细观察的彗星，提供了第一手的彗核结构与彗发和彗尾形成机制的资料。这些观测支持一些长期以来有关彗星结构的假设，特别是弗雷德·惠普的“脏雪球”模型，正确的推测哈雷彗星是挥发性冰－像是水、二氧化碳、和氨－和尘埃的混合物。这个任务提供的资料还大幅改革和重新配置这些材料的想法；例如，理解哈雷彗星的表面主要是布满尘土的，没有挥发性物质，并且只有一小部分是冰。猎户座流星雨是由著名的哈雷彗星造成的。'\n",
        "         }\n",
        "    ],\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "# calculate the time it took to receive the response\n",
        "response_time = time.time() - start_time\n",
        "\n",
        "# print the time delay and text received\n",
        "print(f\"Full response received {response_time:.2f} seconds after request\")\n",
        "print(f\"Full response received:\\n{response}\")\n",
        "print(f\"Full response text:\\n{response.choices[0].message.content}\")"
      ],
      "metadata": {
        "id": "6U6HX6rr5aA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "job_id=\"ftjob-MnBYCcyb27VCgEnJk6mhnrsl\"\n",
        "response = openai.FineTuningJob.retrieve(job_id)\n",
        "fine_tuned_model_id = response[\"fine_tuned_model\"]\n",
        "\n",
        "if fine_tuned_model_id is None:\n",
        "    raise RuntimeError(\"Fine-tuned model ID not found. Your job has likely not been completed yet.\")\n",
        "\n",
        "print(\"Fine-tuned model ID:\", fine_tuned_model_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPDEOUjhOMic",
        "outputId": "0d7f526c-50d9-4d69-b742-8d2d5e2ffa50"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuned model ID: ft:gpt-3.5-turbo-0613:personal:recipe-ner:81S5HAtS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_messages= [{'content': 'You are a helpful assistant.',\n",
        "  'role': 'system'},\n",
        " {'content': 'How do i know a new classmate',\n",
        "  'role': 'user'}]\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=fine_tuned_model_id, messages=test_messages, temperature=0, max_tokens=500\n",
        ")\n",
        "print(response[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7kOpZOWOpIc",
        "outputId": "f01cb474-8c90-4e6d-9a09-db94199651cd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are some tips for getting to know a new classmate:\n",
            "\n",
            "1. Introduce yourself: Take the initiative to introduce yourself to your new classmate. A simple \"Hi, I'm [your name], what's your name?\" can go a long way in starting a conversation.\n",
            "\n",
            "2. Find common interests: Look for common interests or hobbies that you both share. This can be a great conversation starter and help you bond over shared activities.\n",
            "\n",
            "3. Ask open-ended questions: Ask open-ended questions to encourage your new classmate to share more about themselves. For example, you could ask about their favorite subjects, hobbies, or what they like to do in their free time.\n",
            "\n",
            "4. Be a good listener: Show genuine interest in what your new classmate has to say. Listen actively and ask follow-up questions to show that you are engaged in the conversation.\n",
            "\n",
            "5. Offer help: If your new classmate seems lost or unsure about something, offer your assistance. This can be a great way to start a conversation and show that you are friendly and approachable.\n",
            "\n",
            "Remember, building new friendships takes time and effort. Be patient and open-minded, and you'll likely develop a great relationship with your new classmate.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vision"
      ],
      "metadata": {
        "id": "SRPfTRtVnEeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(model=\"gpt-4-vision-preview\", max_tokens=256)\n",
        "chat.invoke(\n",
        "    [\n",
        "        HumanMessage(\n",
        "            content=[\n",
        "                {\"type\": \"text\", \"text\": \"What is this image showing\"},\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\n",
        "                        \"url\": \"https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/static/img/langchain_stack.png\",\n",
        "                        \"detail\": \"auto\",\n",
        "                    },\n",
        "                },\n",
        "            ]\n",
        "        )\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "XzlyKET7nD1L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d809c692-d48e-410d-a4f2-a50c070d103b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='This image appears to be a graphical representation or a diagram of a software architecture or framework related to language processing or a related field. The image is divided into several sections that describe different components and aspects of this system.\\n\\nHere are the visible sections and their respective elements:\\n\\n1. Observability\\n   - LangSmith\\n\\n2. Deployment\\n   - LangServe: Chains as Rest APIs, Reference Applications\\n\\n3. Application\\n   - Templates\\n\\n4. LangChain\\n   - Application: Chains, Agents, Agent Executors\\n   - Integrations Components:\\n       - Models I/O: Model, Prompt, Example Selector, Output Parser\\n       - Retrieval: Retriever, Document Loader, Vector Store, Text Splitter, Embedding Model\\n       - Agent Tooling\\n   - Protocol: LCEL (Parallelization, Fallbacks, Tracing, Batching, Streaming, Async, Composition)\\n   - It also indicates support for Python and JavaScript (indicated by the logos), implying that this framework or system can be utilized with these programming languages.\\n\\nOn the right side of the image, there is a sidebar that lists additional features related to the system:\\n   - Monitoring\\n   - Evaluation\\n   - Annotation\\n   - Feedback\\n   - Testing\\n   - Debugging')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## assistant api"
      ],
      "metadata": {
        "id": "K0ro58bks7F8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### using openai tool"
      ],
      "metadata": {
        "id": "9ALO7LmRuiSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.openai_assistant import OpenAIAssistantRunnable\n"
      ],
      "metadata": {
        "id": "vcSwezbnsb4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter_assistant = OpenAIAssistantRunnable.create_assistant(\n",
        "    name=\"langchain assistant\",\n",
        "    instructions=\"请阅读下面这段文章，并回答哈雷彗星最近一次回归是哪一年？文章如下：哈雷彗星是唯一能用裸眼直接从地球看见的短周期彗星，也是人一生中唯一以裸眼可能看见两次的彗星。其它能以裸眼看见的彗星可能会更壮观和更美丽，但那些都是数千年才会出现一次的彗星。哈雷彗星上一次回归是在1986年，而下一次回归将在2030年中。在1986年回归时，哈雷彗星成为第一颗被宇宙飞船详细观察的彗星，提供了第一手的彗核结构与彗发和彗尾形成机制的资料。这些观测支持一些长期以来有关彗星结构的假设，特别是弗雷德·惠普的“脏雪球”模型，正确的推测哈雷彗星是挥发性冰－像是水、二氧化碳、和氨－和尘埃的混合物。这个任务提供的资料还大幅改革和重新配置这些材料的想法；例如，理解哈雷彗星的表面主要是布满尘土的，没有挥发性物质，并且只有一小部分是冰。猎户座流星雨是由著名的哈雷彗星造成的。\",\n",
        "    tools=[{\"type\": \"code_interpreter\"}],\n",
        "    model=\"gpt-4-1106-preview\"\n",
        ")\n",
        "output = interpreter_assistant.invoke({\"content\": \"What's 10 - 4 raised to the 2.7\"})\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "b3FksUJjsfAM",
        "outputId": "56469bd2-cbf8-45f2-ec87-3db08bf70b7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-3e517d7ab322>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m interpreter_assistant = OpenAIAssistantRunnable.create_assistant(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"langchain assistant\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minstructions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"请阅读下面这段文章，并回答哈雷彗星最近一次回归是哪一年？文章如下：哈雷彗星是唯一能用裸眼直接从地球看见的短周期彗星，也是人一生中唯一以裸眼可能看见两次的彗星。其它能以裸眼看见的彗星可能会更壮观和更美丽，但那些都是数千年才会出现一次的彗星。哈雷彗星上一次回归是在1986年，而下一次回归将在2030年中。在1986年回归时，哈雷彗星成为第一颗被宇宙飞船详细观察的彗星，提供了第一手的彗核结构与彗发和彗尾形成机制的资料。这些观测支持一些长期以来有关彗星结构的假设，特别是弗雷德·惠普的“脏雪球”模型，正确的推测哈雷彗星是挥发性冰－像是水、二氧化碳、和氨－和尘埃的混合物。这个任务提供的资料还大幅改革和重新配置这些材料的想法；例如，理解哈雷彗星的表面主要是布满尘土的，没有挥发性物质，并且只有一小部分是冰。猎户座流星雨是由著名的哈雷彗星造成的。\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtools\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"code_interpreter\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4-1106-preview\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_experimental/openai_assistant/base.py\u001b[0m in \u001b[0;36mcreate_assistant\u001b[0;34m(cls, name, instructions, tools, model, client, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m                 }\n\u001b[1;32m    178\u001b[0m             \u001b[0mopenai_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         assistant = client.beta.assistants.create(\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0minstructions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'OpenAI' object has no attribute 'beta'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### As a LangChain agent with arbitrary tools"
      ],
      "metadata": {
        "id": "GCltKqaRult9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install e2b"
      ],
      "metadata": {
        "id": "NIZUxDRfurfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import E2BDataAnalysisTool\n",
        "\n",
        "tools = [E2BDataAnalysisTool(api_key=\"e2b_8d7e3de7412ebbab1fcadae12421c54bd045ba0c\")]\n",
        "agent = OpenAIAssistantRunnable.create_assistant(\n",
        "    name=\"langchain assistant e2b tool\",\n",
        "    instructions=\"You are a personal math tutor. Write and run code to answer math questions.\",\n",
        "    tools=tools,\n",
        "    model=\"gpt-4-1106-preview\",\n",
        "    as_agent=True\n",
        ")"
      ],
      "metadata": {
        "id": "TBexhQ_1u6CA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using AgentExecutor"
      ],
      "metadata": {
        "id": "j3FhlXGGvAtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor\n",
        "\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
        "agent_executor.invoke({\"content\": \"What's 10 - 4 raised to the 2.7\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTOBthkwvBkV",
        "outputId": "a54168b5-4160-4ec3-8e58-4faa8419cffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'content': \"What's 10 - 4 raised to the 2.7\",\n",
              " 'output': 'The result of the expression \\\\( 10 - 4^{2.7} \\\\) is approximately -32.22425314473263.'}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom execution"
      ],
      "metadata": {
        "id": "63PCwte1vSDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = OpenAIAssistantRunnable.create_assistant(\n",
        "    name=\"langchain assistant e2b tool\",\n",
        "    instructions=\"You are a personal math tutor. Write and run code to answer math questions.\",\n",
        "    tools=tools,\n",
        "    model=\"gpt-4-1106-preview\",\n",
        "    as_agent=True\n",
        ")"
      ],
      "metadata": {
        "id": "Z60VK8hEvBox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.agent import AgentFinish\n",
        "\n",
        "def execute_agent(agent, tools, input):\n",
        "    tool_map = {tool.name: tool for tool in tools}\n",
        "    response = agent.invoke(input)\n",
        "    while not isinstance(response, AgentFinish):\n",
        "        tool_outputs = []\n",
        "        for action in response:\n",
        "            tool_output = tool_map[action.tool].invoke(action.tool_input)\n",
        "            print(action.tool, action.tool_input, tool_output, end=\"\\n\\n\")\n",
        "            tool_outputs.append({\"output\": tool_output, \"tool_call_id\": action.tool_call_id})\n",
        "        response = agent.invoke({\"tool_outputs\": tool_outputs, \"run_id\": action.run_id, \"thread_id\": action.thread_id})\n",
        "\n",
        "    return response\n",
        "\n",
        "response = execute_agent(agent, tools, {\"content\": \"What's 10 - 4 raised to the 2.7\"})\n",
        "print(response.return_values[\"output\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_TLs6XivZYk",
        "outputId": "8d698634-a4c2-415e-d876-21f7ba73b6ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e2b_data_analysis {'python_code': 'result = 10 - 4**2.7\\nprint(result)'} {\"stdout\": \"-32.22425314473263\", \"stderr\": \"\"}\n",
            "\n",
            "The result of \\(10 - 4^{2.7}\\) is approximately \\(-32.224\\).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next_response = execute_agent(agent, tools, {\"content\": \"now add 17.241\", \"thread_id\": response.thread_id})\n",
        "print(next_response.return_values[\"output\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xT2HRTnwN9c",
        "outputId": "21f7d4ce-e01a-42ce-b7c2-4531b1b49547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e2b_data_analysis {'python_code': 'result = 10 - 4**2.7 + 17.241\\nprint(result)'} {\"stdout\": \"-14.983253144732629\", \"stderr\": \"\"}\n",
            "\n",
            "After adding \\(17.241\\) to the previous result, the new result is approximately \\(-14.983\\).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## JSON mode"
      ],
      "metadata": {
        "id": "zEIbHerOyMT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(model=\"gpt-3.5-turbo-1106\").bind(\n",
        "    response_format={\"type\": \"json_object\"}\n",
        ")\n",
        "\n",
        "output = chat.invoke(\n",
        "    [\n",
        "        SystemMessage(\n",
        "            content=\"Extract the 'name' and 'origin' of any companies mentioned in the following statement. Return a JSON list.\"\n",
        "        ),\n",
        "        HumanMessage(\n",
        "            content=\"Google was founded in the USA, while Deepmind was founded in the UK\"\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "print(output.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "ay6YcD4GyN7h",
        "outputId": "b19fe0fa-dc24-417d-8013-695fcedbbaaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-ce93ac4866cf>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m chat = ChatOpenAI(model=\"gpt-3.5-turbo-1106\").bind(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mresponse_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"json_object\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m output = chat.invoke(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lc_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/main.cpython-310-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatOpenAI\n__root__\n  `openai` has no `ChatCompletion` attribute, this is likely due to an old version of the openai package. Try upgrading it with `pip install --upgrade openai`. (type=value_error)"
          ]
        }
      ]
    }
  ]
}