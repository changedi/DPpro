{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaiOaTGmROVF4Rt6crpaYT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changedi/DPpro/blob/master/TransformerDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TbX9zvj_6EVR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.init import xavier_uniform_\n",
        "from torch.nn.init import constant_\n",
        "from torch.nn.init import xavier_normal_\n",
        "import torch.nn.functional as F\n",
        "from typing import Optional, Tuple, Any\n",
        "from typing import List, Optional, Tuple\n",
        "import math\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.词嵌入"
      ],
      "metadata": {
        "id": "fsX_veQ36P4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.zeros((2,4),dtype=torch.long)\n",
        "embed = nn.Embedding(10,8)\n",
        "print(embed(X).shape)\n",
        "\n",
        "X"
      ],
      "metadata": {
        "id": "OQNFKf4f6Sc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.位置编码"
      ],
      "metadata": {
        "id": "79F5Qh2V6qE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Tensor = torch.Tensor\n",
        "def positional_encoding(X, num_features, dropout_p=0.1, max_len=512) -> Tensor:\n",
        "    r'''\n",
        "        给输入加入位置编码\n",
        "    参数：\n",
        "        - num_features: 输入进来的维度\n",
        "        - dropout_p: dropout的概率，当其为非零时执行dropout\n",
        "        - max_len: 句子的最大长度，默认512\n",
        "\n",
        "    形状：\n",
        "        - 输入： [batch_size, seq_length, num_features]\n",
        "        - 输出： [batch_size, seq_length, num_features]\n",
        "\n",
        "    例子：\n",
        "        >>> X = torch.randn((2,4,10))\n",
        "        >>> X = positional_encoding(X, 10)\n",
        "        >>> print(X.shape)\n",
        "        >>> torch.Size([2, 4, 10])\n",
        "    '''\n",
        "\n",
        "    dropout = nn.Dropout(dropout_p)\n",
        "    P = torch.zeros((1,max_len,num_features))\n",
        "    X_ = torch.arange(max_len,dtype=torch.float32).reshape(-1,1) / torch.pow(\n",
        "        10000,\n",
        "        torch.arange(0,num_features,2,dtype=torch.float32) /num_features)\n",
        "    P[:,:,0::2] = torch.sin(X_)\n",
        "    P[:,:,1::2] = torch.cos(X_)\n",
        "    X = X + P[:,:X.shape[1],:].to(X.device)\n",
        "    return dropout(X)"
      ],
      "metadata": {
        "id": "kPROpib26wtg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 位置编码例子\n",
        "X = torch.randn((2,4,10))\n",
        "X = positional_encoding(X, 10)\n",
        "print(X.shape)\n",
        "X"
      ],
      "metadata": {
        "id": "GwwZiDvt62UI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.多头注意力"
      ],
      "metadata": {
        "id": "YnHDAm0J7dUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "点积注意力"
      ],
      "metadata": {
        "id": "i79wv-bX81kY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, Tuple, Any\n",
        "def _scaled_dot_product_attention(\n",
        "    q: Tensor,\n",
        "    k: Tensor,\n",
        "    v: Tensor,\n",
        "    attn_mask: Optional[Tensor] = None,\n",
        "    dropout_p: float = 0.0,\n",
        ") -> Tuple[Tensor, Tensor]:\n",
        "    r'''\n",
        "    在query, key, value上计算点积注意力，若有注意力遮盖则使用，并且应用一个概率为dropout_p的dropout\n",
        "\n",
        "    参数：\n",
        "        - q: shape:`(B, Nt, E)` B代表batch size， Nt是目标语言序列长度，E是嵌入后的特征维度\n",
        "        - key: shape:`(B, Ns, E)` Ns是源语言序列长度\n",
        "        - value: shape:`(B, Ns, E)`与key形状一样\n",
        "        - attn_mask: 要么是3D的tensor，形状为:`(B, Nt, Ns)`或者2D的tensor，形状如:`(Nt, Ns)`\n",
        "\n",
        "        - Output: attention values: shape:`(B, Nt, E)`，与q的形状一致;attention weights: shape:`(B, Nt, Ns)`\n",
        "\n",
        "    例子：\n",
        "        >>> q = torch.randn((2,3,6))\n",
        "        >>> k = torch.randn((2,4,6))\n",
        "        >>> v = torch.randn((2,4,6))\n",
        "        >>> out = scaled_dot_product_attention(q, k, v)\n",
        "        >>> out[0].shape, out[1].shape\n",
        "        >>> torch.Size([2, 3, 6]) torch.Size([2, 3, 4])\n",
        "    '''\n",
        "    B, Nt, E = q.shape\n",
        "    q = q / math.sqrt(E)\n",
        "    # (B, Nt, E) x (B, E, Ns) -> (B, Nt, Ns)\n",
        "    attn = torch.bmm(q, k.transpose(-2,-1))\n",
        "    if attn_mask is not None:\n",
        "        attn += attn_mask\n",
        "    # attn意味着目标序列的每个词对源语言序列做注意力\n",
        "    attn = F.softmax(attn, dim=-1)\n",
        "    if dropout_p:\n",
        "        attn = F.dropout(attn, p=dropout_p)\n",
        "    # (B, Nt, Ns) x (B, Ns, E) -> (B, Nt, E)\n",
        "    output = torch.bmm(attn, v)\n",
        "    return output, attn\n"
      ],
      "metadata": {
        "id": "UreDW4t28u_E"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "q,k,v变换"
      ],
      "metadata": {
        "id": "lOC1wFHy85RQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _in_projection_packed(\n",
        "    q: Tensor,\n",
        "    k: Tensor,\n",
        "    v: Tensor,\n",
        "    w: Tensor,\n",
        "    b: Optional[Tensor] = None,\n",
        ") -> List[Tensor]:\n",
        "    r\"\"\"\n",
        "    用一个大的权重参数矩阵进行线性变换\n",
        "\n",
        "    参数:\n",
        "        q, k, v: 对自注意来说，三者都是src；对于seq2seq模型，k和v是一致的tensor。\n",
        "                 但它们的最后一维(num_features或者叫做embed_dim)都必须保持一致。\n",
        "        w: 用以线性变换的大矩阵，按照q,k,v的顺序压在一个tensor里面。\n",
        "        b: 用以线性变换的偏置，按照q,k,v的顺序压在一个tensor里面。\n",
        "\n",
        "    形状:\n",
        "        输入:\n",
        "        - q: shape:`(..., E)`，E是词嵌入的维度（下面出现的E均为此意）。\n",
        "        - k: shape:`(..., E)`\n",
        "        - v: shape:`(..., E)`\n",
        "        - w: shape:`(E * 3, E)`\n",
        "        - b: shape:`E * 3`\n",
        "\n",
        "        输出:\n",
        "        - 输出列表 :`[q', k', v']`，q,k,v经过线性变换前后的形状都一致。\n",
        "    \"\"\"\n",
        "    E = q.size(-1)\n",
        "    # 若为自注意，则q = k = v = src，因此它们的引用变量都是src\n",
        "    # 即k is v和q is k结果均为True\n",
        "    # 若为seq2seq，k = v，因而k is v的结果是True\n",
        "    if k is v:\n",
        "        if q is k:\n",
        "            return F.linear(q, w, b).chunk(3, dim=-1)\n",
        "        else:\n",
        "            # seq2seq模型\n",
        "            w_q, w_kv = w.split([E, E * 2])\n",
        "            if b is None:\n",
        "                b_q = b_kv = None\n",
        "            else:\n",
        "                b_q, b_kv = b.split([E, E * 2])\n",
        "            return (F.linear(q, w_q, b_q),) + F.linear(k, w_kv, b_kv).chunk(2, dim=-1)\n",
        "    else:\n",
        "        w_q, w_k, w_v = w.chunk(3)\n",
        "        if b is None:\n",
        "            b_q = b_k = b_v = None\n",
        "        else:\n",
        "            b_q, b_k, b_v = b.chunk(3)\n",
        "        return F.linear(q, w_q, b_q), F.linear(k, w_k, b_k), F.linear(v, w_v, b_v)\n",
        "\n",
        "# q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)"
      ],
      "metadata": {
        "id": "G_SjYEwX9FKK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.定义multihead_attention_forward"
      ],
      "metadata": {
        "id": "2okAcBk37f19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Tensor = torch.Tensor\n",
        "def multi_head_attention_forward(\n",
        "    query: Tensor,\n",
        "    key: Tensor,\n",
        "    value: Tensor,\n",
        "    num_heads: int,\n",
        "    in_proj_weight: Tensor,\n",
        "    in_proj_bias: Optional[Tensor],\n",
        "    dropout_p: float,\n",
        "    out_proj_weight: Tensor,\n",
        "    out_proj_bias: Optional[Tensor],\n",
        "    training: bool = True,\n",
        "    key_padding_mask: Optional[Tensor] = None,\n",
        "    need_weights: bool = True,\n",
        "    attn_mask: Optional[Tensor] = None,\n",
        "    use_seperate_proj_weight = None,\n",
        "    q_proj_weight: Optional[Tensor] = None,\n",
        "    k_proj_weight: Optional[Tensor] = None,\n",
        "    v_proj_weight: Optional[Tensor] = None,\n",
        ") -> Tuple[Tensor, Optional[Tensor]]:\n",
        "    r'''\n",
        "    形状：\n",
        "        输入：\n",
        "        - query：`(L, N, E)`\n",
        "        - key: `(S, N, E)`\n",
        "        - value: `(S, N, E)`\n",
        "        - key_padding_mask: `(N, S)`\n",
        "        - attn_mask: `(L, S)` or `(N * num_heads, L, S)`\n",
        "        输出：\n",
        "        - attn_output:`(L, N, E)`\n",
        "        - attn_output_weights:`(N, L, S)`\n",
        "    '''\n",
        "    tgt_len, bsz, embed_dim = query.shape\n",
        "    src_len, _, _ = key.shape\n",
        "    head_dim = embed_dim // num_heads\n",
        "    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n",
        "\n",
        "    if attn_mask is not None:\n",
        "        if attn_mask.dtype == torch.uint8:\n",
        "            warnings.warn(\"Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
        "            attn_mask = attn_mask.to(torch.bool)\n",
        "        else:\n",
        "            assert attn_mask.is_floating_point() or attn_mask.dtype == torch.bool, \\\n",
        "                f\"Only float, byte, and bool types are supported for attn_mask, not {attn_mask.dtype}\"\n",
        "\n",
        "        if attn_mask.dim() == 2:\n",
        "            correct_2d_size = (tgt_len, src_len)\n",
        "            if attn_mask.shape != correct_2d_size:\n",
        "                raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n",
        "            attn_mask = attn_mask.unsqueeze(0)\n",
        "        elif attn_mask.dim() == 3:\n",
        "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
        "            if attn_mask.shape != correct_3d_size:\n",
        "                raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n",
        "        else:\n",
        "            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
        "\n",
        "    if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n",
        "        warnings.warn(\"Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
        "        key_padding_mask = key_padding_mask.to(torch.bool)\n",
        "\n",
        "    # reshape q,k,v将Batch放在第一维以适合点积注意力\n",
        "    # 同时为多头机制，将不同的头拼在一起组成一层\n",
        "    q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
        "    k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
        "    v = v.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
        "    if key_padding_mask is not None:\n",
        "        assert key_padding_mask.shape == (bsz, src_len), \\\n",
        "            f\"expecting key_padding_mask shape of {(bsz, src_len)}, but got {key_padding_mask.shape}\"\n",
        "        key_padding_mask = key_padding_mask.view(bsz, 1, 1, src_len).   \\\n",
        "            expand(-1, num_heads, -1, -1).reshape(bsz * num_heads, 1, src_len)\n",
        "        if attn_mask is None:\n",
        "            attn_mask = key_padding_mask\n",
        "        elif attn_mask.dtype == torch.bool:\n",
        "            attn_mask = attn_mask.logical_or(key_padding_mask)\n",
        "        else:\n",
        "            attn_mask = attn_mask.masked_fill(key_padding_mask, float(\"-inf\"))\n",
        "    # 若attn_mask值是布尔值，则将mask转换为float\n",
        "    if attn_mask is not None and attn_mask.dtype == torch.bool:\n",
        "        new_attn_mask = torch.zeros_like(attn_mask, dtype=torch.float)\n",
        "        new_attn_mask.masked_fill_(attn_mask, float(\"-inf\"))\n",
        "        attn_mask = new_attn_mask\n",
        "\n",
        "    # 若training为True时才应用dropout\n",
        "    if not training:\n",
        "        dropout_p = 0.0\n",
        "    attn_output, attn_output_weights = _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)\n",
        "    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
        "    attn_output = nn.functional.linear(attn_output, out_proj_weight, out_proj_bias)\n",
        "    if need_weights:\n",
        "        # average attention weights over heads\n",
        "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
        "        return attn_output, attn_output_weights.sum(dim=1) / num_heads\n",
        "    else:\n",
        "        return attn_output, None"
      ],
      "metadata": {
        "id": "vBiCEpLq8h15"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.完整的多头注意力层"
      ],
      "metadata": {
        "id": "W1qaLak09Nsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "    r'''\n",
        "    参数：\n",
        "        embed_dim: 词嵌入的维度\n",
        "        num_heads: 平行头的数量\n",
        "        batch_first: 若`True`，则为(batch, seq, feture)，若为`False`，则为(seq, batch, feature)\n",
        "\n",
        "    例子：\n",
        "        >>> multihead_attn = MultiheadAttention(embed_dim, num_heads)\n",
        "        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
        "    '''\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0., bias=True,\n",
        "                 kdim=None, vdim=None, batch_first=False) -> None:\n",
        "        # factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        super(MultiheadAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.kdim = kdim if kdim is not None else embed_dim\n",
        "        self.vdim = vdim if vdim is not None else embed_dim\n",
        "        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.batch_first = batch_first\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        if self._qkv_same_embed_dim is False:\n",
        "            self.q_proj_weight = Parameter(torch.empty((embed_dim, embed_dim)))\n",
        "            self.k_proj_weight = Parameter(torch.empty((embed_dim, self.kdim)))\n",
        "            self.v_proj_weight = Parameter(torch.empty((embed_dim, self.vdim)))\n",
        "            self.register_parameter('in_proj_weight', None)\n",
        "        else:\n",
        "            self.in_proj_weight = Parameter(torch.empty((3 * embed_dim, embed_dim)))\n",
        "            self.register_parameter('q_proj_weight', None)\n",
        "            self.register_parameter('k_proj_weight', None)\n",
        "            self.register_parameter('v_proj_weight', None)\n",
        "\n",
        "        if bias:\n",
        "            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim))\n",
        "        else:\n",
        "            self.register_parameter('in_proj_bias', None)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        if self._qkv_same_embed_dim:\n",
        "            xavier_uniform_(self.in_proj_weight)\n",
        "        else:\n",
        "            xavier_uniform_(self.q_proj_weight)\n",
        "            xavier_uniform_(self.k_proj_weight)\n",
        "            xavier_uniform_(self.v_proj_weight)\n",
        "\n",
        "        if self.in_proj_bias is not None:\n",
        "            constant_(self.in_proj_bias, 0.)\n",
        "            constant_(self.out_proj.bias, 0.)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, query: Tensor, key: Tensor, value: Tensor, key_padding_mask: Optional[Tensor] = None,\n",
        "                need_weights: bool = True, attn_mask: Optional[Tensor] = None) -> Tuple[Tensor, Optional[Tensor]]:\n",
        "        if self.batch_first:\n",
        "            query, key, value = [x.transpose(1, 0) for x in (query, key, value)]\n",
        "\n",
        "        if not self._qkv_same_embed_dim:\n",
        "            attn_output, attn_output_weights = multi_head_attention_forward(\n",
        "                query, key, value, self.num_heads,\n",
        "                self.in_proj_weight, self.in_proj_bias,\n",
        "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
        "                training=self.training,\n",
        "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
        "                attn_mask=attn_mask, use_separate_proj_weight=True,\n",
        "                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n",
        "                v_proj_weight=self.v_proj_weight)\n",
        "        else:\n",
        "            attn_output, attn_output_weights = multi_head_attention_forward(\n",
        "                query, key, value, self.num_heads,\n",
        "                self.in_proj_weight, self.in_proj_bias,\n",
        "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
        "                training=self.training,\n",
        "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
        "                attn_mask=attn_mask)\n",
        "        if self.batch_first:\n",
        "            return attn_output.transpose(1, 0), attn_output_weights\n",
        "        else:\n",
        "            return attn_output, attn_output_weights"
      ],
      "metadata": {
        "id": "QRt-gBK87fDE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 因为batch_first为False,所以src的shape：`(seq, batch, embed_dim)`\n",
        "src = torch.randn((2,4,100))\n",
        "src = positional_encoding(src,100,0.1)\n",
        "print(src.shape)\n",
        "multihead_attn = MultiheadAttention(100, 4, 0.1)\n",
        "attn_output, attn_output_weights = multihead_attn(src,src,src)\n",
        "print(attn_output.shape, attn_output_weights.shape)\n",
        "\n",
        "# torch.Size([2, 4, 100])\n",
        "# torch.Size([2, 4, 100]) torch.Size([4, 2, 2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLcCvTDr8N67",
        "outputId": "e3149a5d-a56d-42fe-a368-72031d27575f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 4, 100])\n",
            "torch.Size([2, 4, 100]) torch.Size([4, 2, 2])\n"
          ]
        }
      ]
    }
  ]
}