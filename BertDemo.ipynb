{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changedi/DPpro/blob/master/BertDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yfoNbGNppuC8"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers==4.4.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqwHqoMHqGB6"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "bt = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bt('I like natural language progressing!')\n",
        "# {'input_ids': [101, 1045, 2066, 3019, 2653, 27673, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HFJ06PFNuqT"
      },
      "source": [
        "## BertForPreTraining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mavI1zBWNAss"
      },
      "outputs": [],
      "source": [
        "_CHECKPOINT_FOR_DOC = \"bert-base-uncased\"\n",
        "_CONFIG_FOR_DOC = \"BertConfig\"\n",
        "_TOKENIZER_FOR_DOC = \"BertTokenizer\"\n",
        "from transformers.models.bert.modeling_bert import *\n",
        "from transformers.models.bert.configuration_bert import *\n",
        "class BertForPreTraining(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.cls = BertPreTrainingHeads(config)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.cls.predictions.decoder\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.cls.predictions.decoder = new_embeddings\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @replace_return_docstrings(output_type=BertForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        next_sentence_label=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape ``(batch_size, sequence_length)``, `optional`):\n",
        "            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n",
        "            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n",
        "            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n",
        "        next_sentence_label (``torch.LongTensor`` of shape ``(batch_size,)``, `optional`):\n",
        "            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair\n",
        "            (see :obj:`input_ids` docstring) Indices should be in ``[0, 1]``:\n",
        "            - 0 indicates sequence B is a continuation of sequence A,\n",
        "            - 1 indicates sequence B is a random sequence.\n",
        "        kwargs (:obj:`Dict[str, any]`, optional, defaults to `{}`):\n",
        "            Used to hide legacy arguments that have been deprecated.\n",
        "        Returns:\n",
        "        Example::\n",
        "from transformers import BertTokenizer, BertForPreTraining\n",
        "import torch\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForPreTraining.from_pretrained('bert-base-uncased')\n",
        "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "prediction_logits = outputs.prediction_logits\n",
        "seq_relationship_logits = outputs.seq_relationship_logits\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output, pooled_output = outputs[:2]\n",
        "        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
        "\n",
        "        total_loss = None\n",
        "        if labels is not None and next_sentence_label is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
        "            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n",
        "            total_loss = masked_lm_loss + next_sentence_loss\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (prediction_scores, seq_relationship_score) + outputs[2:]\n",
        "            return ((total_loss,) + output) if total_loss is not None else output\n",
        "\n",
        "        return BertForPreTrainingOutput(\n",
        "            loss=total_loss,\n",
        "            prediction_logits=prediction_scores,\n",
        "            seq_relationship_logits=seq_relationship_score,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "from transformers import BertTokenizer, BertForPreTraining\n",
        "import torch\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForPreTraining.from_pretrained('bert-base-uncased')\n",
        "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "prediction_logits = outputs.prediction_logits\n",
        "seq_relationship_logits = outputs.seq_relationship_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhKOro94NPOz"
      },
      "outputs": [],
      "source": [
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaOMJB2IfZTZ"
      },
      "source": [
        "### BertLMHeadModelÔºöËøô‰∏™Âíå‰∏ä‰∏Ä‰∏™ÁöÑÂå∫Âà´Âú®‰∫éÔºåËøô‰∏ÄÊ®°ÂûãÊòØ‰Ωú‰∏∫ decoder ËøêË°åÁöÑÁâàÊú¨Ôºõ\n",
        "ÂêåÊ†∑Âü∫‰∫éBertOnlyMLMHeadÔºõ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASvMkR48N_jx"
      },
      "outputs": [],
      "source": [
        "@add_start_docstrings(\n",
        "    \"\"\"Bert Model with a `language modeling` head on top for CLM fine-tuning. \"\"\", BERT_START_DOCSTRING\n",
        ")\n",
        "class BertLMHeadModel(BertPreTrainedModel):\n",
        "\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        if not config.is_decoder:\n",
        "            logger.warning(\"If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\")\n",
        "\n",
        "        self.bert = BertModel(config, add_pooling_layer=False)\n",
        "        self.cls = BertOnlyMLMHead(config)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.cls.predictions.decoder\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.cls.predictions.decoder = new_embeddings\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        labels=None,\n",
        "        past_key_values=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
        "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
        "            the model is configured as a decoder.\n",
        "        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
        "            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n",
        "            ``[-100, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are\n",
        "            ignored (masked), the loss is only computed for the tokens with labels n ``[0, ..., config.vocab_size]``\n",
        "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
        "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
        "            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n",
        "            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n",
        "            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n",
        "        use_cache (:obj:`bool`, `optional`):\n",
        "            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n",
        "            decoding (see :obj:`past_key_values`).\n",
        "        Returns:\n",
        "        Example::\n",
        "            from transformers import BertTokenizer, BertLMHeadModel, BertConfig\n",
        "            import torch\n",
        "            tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "            config = BertConfig.from_pretrained(\"bert-base-cased\")\n",
        "            config.is_decoder = True\n",
        "            model = BertLMHeadModel.from_pretrained('bert-base-cased', config=config)\n",
        "            inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "            outputs = model(**inputs)\n",
        "            prediction_logits = outputs.logits\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        if labels is not None:\n",
        "            use_cache = False\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        prediction_scores = self.cls(sequence_output)\n",
        "\n",
        "        lm_loss = None\n",
        "        if labels is not None:\n",
        "            # we are doing next-token prediction; shift prediction scores and input ids by one\n",
        "            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n",
        "            labels = labels[:, 1:].contiguous()\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (prediction_scores,) + outputs[2:]\n",
        "            return ((lm_loss,) + output) if lm_loss is not None else output\n",
        "\n",
        "        return CausalLMOutputWithCrossAttentions(\n",
        "            loss=lm_loss,\n",
        "            logits=prediction_scores,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "            cross_attentions=outputs.cross_attentions,\n",
        "        )\n",
        "\n",
        "    def prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, **model_kwargs):\n",
        "        input_shape = input_ids.shape\n",
        "        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n",
        "        if attention_mask is None:\n",
        "            attention_mask = input_ids.new_ones(input_shape)\n",
        "\n",
        "        # cut decoder_input_ids if past is used\n",
        "        if past is not None:\n",
        "            input_ids = input_ids[:, -1:]\n",
        "\n",
        "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"past_key_values\": past}\n",
        "\n",
        "    def _reorder_cache(self, past, beam_idx):\n",
        "        reordered_past = ()\n",
        "        for layer_past in past:\n",
        "            reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n",
        "        return reordered_past\n",
        "\n",
        "from transformers import BertTokenizer, BertLMHeadModel, BertConfig\n",
        "import torch\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
        "config.is_decoder = True\n",
        "model = BertLMHeadModel.from_pretrained('bert-base-uncased', config=config)\n",
        "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "prediction_logits = outputs.logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AI75Vp3HelTw"
      },
      "outputs": [],
      "source": [
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HE5Ini9KfhL9"
      },
      "source": [
        "### BertForNextSentencePredictionÔºöÂè™ËøõË°å NSP ‰ªªÂä°ÁöÑÈ¢ÑËÆ≠ÁªÉ„ÄÇ\n",
        "Âü∫‰∫éBertOnlyNSPHeadÔºåÂÜÖÂÆπÂ∞±ÊòØ‰∏Ä‰∏™Á∫øÊÄßÂ±Ç„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUs0dRq7e6DE"
      },
      "outputs": [],
      "source": [
        "class BertForNextSentencePrediction(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.cls = BertOnlyNSPHead(config)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @replace_return_docstrings(output_type=NextSentencePredictorOutput, config_class=_CONFIG_FOR_DOC)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair\n",
        "            (see ``input_ids`` docstring). Indices should be in ``[0, 1]``:\n",
        "            - 0 indicates sequence B is a continuation of sequence A,\n",
        "            - 1 indicates sequence B is a random sequence.\n",
        "        Returns:\n",
        "        Example::\n",
        "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
        "import torch\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
        "prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
        "next_sentence = \"The sky is blue due to the shorter wavelength of blue light.\"\n",
        "encoding = tokenizer(prompt, next_sentence, return_tensors='pt')\n",
        "outputs = model(**encoding, labels=torch.LongTensor([1]))\n",
        "logits = outputs.logits\n",
        "assert logits[0, 0] < logits[0, 1] # next sentence was random\n",
        "        \"\"\"\n",
        "\n",
        "        if \"next_sentence_label\" in kwargs:\n",
        "            warnings.warn(\n",
        "                \"The `next_sentence_label` argument is deprecated and will be removed in a future version, use `labels` instead.\",\n",
        "                FutureWarning,\n",
        "            )\n",
        "            labels = kwargs.pop(\"next_sentence_label\")\n",
        "\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        seq_relationship_scores = self.cls(pooled_output)\n",
        "\n",
        "        next_sentence_loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            next_sentence_loss = loss_fct(seq_relationship_scores.view(-1, 2), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (seq_relationship_scores,) + outputs[2:]\n",
        "            return ((next_sentence_loss,) + output) if next_sentence_loss is not None else output\n",
        "\n",
        "        return NextSentencePredictorOutput(\n",
        "            loss=next_sentence_loss,\n",
        "            logits=seq_relationship_scores,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
        "import torch\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
        "prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
        "next_sentence = \"The sky is blue due to the shorter wavelength of blue light.\"\n",
        "encoding = tokenizer(prompt, next_sentence, return_tensors='pt')\n",
        "outputs = model(**encoding, labels=torch.LongTensor([1]))\n",
        "logits = outputs.logits\n",
        "assert logits[0, 0] < logits[0, 1] # next sentence was random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IY0uJu8Ke9tL",
        "outputId": "5ab84c1b-097c-4fa4-911d-d6cfbef42a64"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "NextSentencePredictorOutput(loss=tensor(0.0001, grad_fn=<NllLossBackward0>), logits=tensor([[-3.0729,  5.9056]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83euq1DvNxry"
      },
      "source": [
        "## BertForSequenceClassification\n",
        "Ëøô‰∏ÄÊ®°ÂûãÁî®‰∫éÂè•Â≠êÂàÜÁ±ªÔºà‰πüÂèØ‰ª•ÊòØÂõûÂΩíÔºâ‰ªªÂä°ÔºåÊØîÂ¶Ç GLUE benchmark ÁöÑÂêÑ‰∏™‰ªªÂä°„ÄÇ\n",
        "\n",
        "Âè•Â≠êÂàÜÁ±ªÁöÑËæìÂÖ•‰∏∫Âè•Â≠êÔºàÂØπÔºâÔºåËæìÂá∫‰∏∫Âçï‰∏™ÂàÜÁ±ªÊ†áÁ≠æ„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCZEG1rZk4sw"
      },
      "outputs": [],
      "source": [
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled\n",
        "    output) e.g. for GLUE tasks.\n",
        "    \"\"\",\n",
        "    BERT_START_DOCSTRING,\n",
        ")\n",
        "class BertForSequenceClassification(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.config = config\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        classifier_dropout = (\n",
        "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
        "        )\n",
        "        self.dropout = nn.Dropout(classifier_dropout)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=SequenceClassifierOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
        "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
        "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.config.problem_type is None:\n",
        "                if self.num_labels == 1:\n",
        "                    self.config.problem_type = \"regression\"\n",
        "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
        "                    self.config.problem_type = \"single_label_classification\"\n",
        "                else:\n",
        "                    self.config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "            if self.config.problem_type == \"regression\":\n",
        "                loss_fct = MSELoss()\n",
        "                if self.num_labels == 1:\n",
        "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
        "                else:\n",
        "                    loss = loss_fct(logits, labels)\n",
        "            elif self.config.problem_type == \"single_label_classification\":\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            elif self.config.problem_type == \"multi_label_classification\":\n",
        "                loss_fct = BCEWithLogitsLoss()\n",
        "                loss = loss_fct(logits, labels)\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BertForMultipleChoice\n",
        "Ëøô‰∏ÄÊ®°ÂûãÁî®‰∫éÂ§öÈ°πÈÄâÊã©ÔºåÂ¶Ç RocStories/SWAG ‰ªªÂä°„ÄÇ\n",
        "\n",
        "Â§öÈ°πÈÄâÊã©‰ªªÂä°ÁöÑËæìÂÖ•‰∏∫‰∏ÄÁªÑÂàÜÊ¨°ËæìÂÖ•ÁöÑÂè•Â≠êÔºåËæìÂá∫‰∏∫ÈÄâÊã©Êüê‰∏ÄÂè•Â≠êÁöÑÂçï‰∏™Ê†áÁ≠æ„ÄÇ ÁªìÊûÑ‰∏ä‰∏éÂè•Â≠êÂàÜÁ±ªÁõ∏‰ººÔºåÂè™‰∏çËøáÁ∫øÊÄßÂ±ÇËæìÂá∫Áª¥Â∫¶‰∏∫ 1ÔºåÂç≥ÊØèÊ¨°ÈúÄË¶ÅÂ∞ÜÊØè‰∏™Ê†∑Êú¨ÁöÑÂ§ö‰∏™Âè•Â≠êÁöÑËæìÂá∫ÊãºÊé•Ëµ∑Êù•‰Ωú‰∏∫ÊØè‰∏™Ê†∑Êú¨ÁöÑÈ¢ÑÊµãÂàÜÊï∞„ÄÇ\n",
        "ÂÆûÈôÖ‰∏äÔºåÂÖ∑‰ΩìÊìç‰ΩúÊó∂ÊòØÊääÊØè‰∏™ batch ÁöÑÂ§ö‰∏™Âè•Â≠ê‰∏ÄÂêåÊîæÂÖ•ÁöÑÔºåÊâÄ‰ª•‰∏ÄÊ¨°Â§ÑÁêÜÁöÑËæìÂÖ•‰∏∫[batch_size, num_choices]Êï∞ÈáèÁöÑÂè•Â≠êÔºåÂõ†Ê≠§Áõ∏Âêå batch Â§ßÂ∞èÊó∂ÔºåÊØîÂè•Â≠êÂàÜÁ±ªÁ≠â‰ªªÂä°ÈúÄË¶ÅÊõ¥Â§öÁöÑÊòæÂ≠òÔºåÂú®ËÆ≠ÁªÉÊó∂ÈúÄË¶ÅÂ∞èÂøÉ„ÄÇ"
      ],
      "metadata": {
        "id": "k_8NfgOSZjpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertForMultipleChoice(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=MultipleChoiceModelOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for computing the multiple choice classification loss. Indices should be in ``[0, ...,\n",
        "            num_choices-1]`` where :obj:`num_choices` is the size of the second dimension of the input tensors. (See\n",
        "            :obj:`input_ids` above)\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n",
        "\n",
        "        input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n",
        "        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n",
        "        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n",
        "        position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n",
        "        inputs_embeds = (\n",
        "            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n",
        "            if inputs_embeds is not None\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        reshaped_logits = logits.view(-1, num_choices)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(reshaped_logits, labels)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (reshaped_logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return MultipleChoiceModelOutput(\n",
        "            loss=loss,\n",
        "            logits=reshaped_logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n"
      ],
      "metadata": {
        "id": "li81JPgdZU9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BertForTokenClassification\n",
        "Ëøô‰∏ÄÊ®°ÂûãÁî®‰∫éÂ∫èÂàóÊ†áÊ≥®ÔºàËØçÂàÜÁ±ªÔºâÔºåÂ¶Ç NER ‰ªªÂä°„ÄÇ\n",
        "\n",
        "Â∫èÂàóÊ†áÊ≥®‰ªªÂä°ÁöÑËæìÂÖ•‰∏∫Âçï‰∏™Âè•Â≠êÊñáÊú¨ÔºåËæìÂá∫‰∏∫ÊØè‰∏™ token ÂØπÂ∫îÁöÑÁ±ªÂà´Ê†áÁ≠æ„ÄÇ Áî±‰∫éÈúÄË¶ÅÁî®Âà∞ÊØè‰∏™ tokenÂØπÂ∫îÁöÑËæìÂá∫ËÄå‰∏çÂè™ÊòØÊüêÂá†‰∏™ÔºåÊâÄ‰ª•ËøôÈáåÁöÑBertModel‰∏çÁî®Âä†ÂÖ• pooling Â±ÇÔºõ\n",
        "ÂêåÊó∂ÔºåËøôÈáåÂ∞Ü_keys_to_ignore_on_load_unexpectedËøô‰∏Ä‰∏™Á±ªÂèÇÊï∞ËÆæÁΩÆ‰∏∫[r\"pooler\"]Ôºå‰πüÂ∞±ÊòØÂú®Âä†ËΩΩÊ®°ÂûãÊó∂ÂØπ‰∫éÂá∫Áé∞‰∏çÈúÄË¶ÅÁöÑÊùÉÈáç‰∏çÂèëÁîüÊä•Èîô„ÄÇ"
      ],
      "metadata": {
        "id": "1o1biOZMYo8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    Bert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n",
        "    Named-Entity-Recognition (NER) tasks.\n",
        "    \"\"\",\n",
        "    BERT_START_DOCSTRING,\n",
        ")\n",
        "class BertForTokenClassification(BertPreTrainedModel):\n",
        "\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.bert = BertModel(config, add_pooling_layer=False)\n",
        "        classifier_dropout = (\n",
        "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
        "        )\n",
        "        self.dropout = nn.Dropout(classifier_dropout)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=TokenClassifierOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n",
        "            1]``.\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            # Only keep active parts of the loss\n",
        "            if attention_mask is not None:\n",
        "                active_loss = attention_mask.view(-1) == 1\n",
        "                active_logits = logits.view(-1, self.num_labels)\n",
        "                active_labels = torch.where(\n",
        "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
        "                )\n",
        "                loss = loss_fct(active_logits, active_labels)\n",
        "            else:\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return TokenClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n"
      ],
      "metadata": {
        "id": "9ViCN60YZYeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForTokenClassification, BertTokenizer\n",
        "import torch\n",
        "\n",
        "model = BertForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "label_list = [\n",
        "\"O\",       # Outside of a named entity\n",
        "\"B-MISC\",  # Beginning of a miscellaneous entity right after another miscellaneous entity\n",
        "\"I-MISC\",  # Miscellaneous entity\n",
        "\"B-PER\",   # Beginning of a person's name right after another person's name\n",
        "\"I-PER\",   # Person's name\n",
        "\"B-ORG\",   # Beginning of an organisation right after another organisation\n",
        "\"I-ORG\",   # Organisation\n",
        "\"B-LOC\",   # Beginning of a location right after another location\n",
        "\"I-LOC\"    # Location\n",
        "]\n",
        "\n",
        "sequence = \"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very close to the Manhattan Bridge.\"\n",
        "\n",
        "# Bit of a hack to get the tokens with the special tokens\n",
        "tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))\n",
        "inputs = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model(inputs).logits\n",
        "predictions = torch.argmax(outputs, dim=2)"
      ],
      "metadata": {
        "id": "xdJACsD7ZvIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token, prediction in zip(tokens, predictions[0].numpy()):\n",
        "    print((token, model.config.id2label[prediction]))"
      ],
      "metadata": {
        "id": "TuwpYMhTZ3CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BertForQuestionAnswering\n",
        "Ëøô‰∏ÄÊ®°ÂûãÁî®‰∫éËß£ÂÜ≥ÈóÆÁ≠î‰ªªÂä°Ôºå‰æãÂ¶Ç SQuAD ‰ªªÂä°„ÄÇ\n",
        "\n",
        "ÈóÆÁ≠î‰ªªÂä°ÁöÑËæìÂÖ•‰∏∫ÈóÆÈ¢ò +ÔºàÂØπ‰∫é BERT Âè™ËÉΩÊòØ‰∏Ä‰∏™ÔºâÂõûÁ≠îÁªÑÊàêÁöÑÂè•Â≠êÂØπÔºåËæìÂá∫‰∏∫Ëµ∑Âßã‰ΩçÁΩÆÂíåÁªìÊùü‰ΩçÁΩÆÁî®‰∫éÊ†áÂá∫ÂõûÁ≠î‰∏≠ÁöÑÂÖ∑‰ΩìÊñáÊú¨„ÄÇ ËøôÈáåÈúÄË¶Å‰∏§‰∏™ËæìÂá∫ÔºåÂç≥ÂØπËµ∑Âßã‰ΩçÁΩÆÁöÑÈ¢ÑÊµãÂíåÂØπÁªìÊùü‰ΩçÁΩÆÁöÑÈ¢ÑÊµãÔºå‰∏§‰∏™ËæìÂá∫ÁöÑÈïøÂ∫¶ÈÉΩÂíåÂè•Â≠êÈïøÂ∫¶‰∏ÄÊ†∑Ôºå‰ªéÂÖ∂‰∏≠ÊåëÂá∫ÊúÄÂ§ßÁöÑÈ¢ÑÊµãÂÄºÂØπÂ∫îÁöÑ‰∏ãÊ†á‰Ωú‰∏∫È¢ÑÊµãÁöÑ‰ΩçÁΩÆ„ÄÇ\n",
        "ÂØπË∂ÖÂá∫Âè•Â≠êÈïøÂ∫¶ÁöÑÈùûÊ≥ï labelÔºå‰ºöÂ∞ÜÂÖ∂ÂéãÁº©Ôºàtorch.clamp_ÔºâÂà∞ÂêàÁêÜËåÉÂõ¥„ÄÇ"
      ],
      "metadata": {
        "id": "KZ2nXmKhZ7CR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    Bert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n",
        "    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n",
        "    \"\"\",\n",
        "    BERT_START_DOCSTRING,\n",
        ")\n",
        "class BertForQuestionAnswering(BertPreTrainedModel):\n",
        "\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.bert = BertModel(config, add_pooling_layer=False)\n",
        "        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=QuestionAnsweringModelOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        start_positions=None,\n",
        "        end_positions=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n",
        "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n",
        "            sequence are not taken into account for computing the loss.\n",
        "        end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n",
        "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n",
        "            sequence are not taken into account for computing the loss.\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        logits = self.qa_outputs(sequence_output)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1).contiguous()\n",
        "        end_logits = end_logits.squeeze(-1).contiguous()\n",
        "\n",
        "        total_loss = None\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            # If we are on multi-GPU, split add a dimension\n",
        "            if len(start_positions.size()) > 1:\n",
        "                start_positions = start_positions.squeeze(-1)\n",
        "            if len(end_positions.size()) > 1:\n",
        "                end_positions = end_positions.squeeze(-1)\n",
        "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
        "            ignored_index = start_logits.size(1)\n",
        "            start_positions = start_positions.clamp(0, ignored_index)\n",
        "            end_positions = end_positions.clamp(0, ignored_index)\n",
        "\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
        "            start_loss = loss_fct(start_logits, start_positions)\n",
        "            end_loss = loss_fct(end_logits, end_positions)\n",
        "            total_loss = (start_loss + end_loss) / 2\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (start_logits, end_logits) + outputs[2:]\n",
        "            return ((total_loss,) + output) if total_loss is not None else output\n",
        "\n",
        "        return QuestionAnsweringModelOutput(\n",
        "            loss=total_loss,\n",
        "            start_logits=start_logits,\n",
        "            end_logits=end_logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n"
      ],
      "metadata": {
        "id": "LaX50tgDaBrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "\n",
        "text = \"ü§ó Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet‚Ä¶) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch.\"\n",
        "\n",
        "questions = [\n",
        "\"How many pretrained models are available in ü§ó Transformers?\",\n",
        "\"What does ü§ó Transformers provide?\",\n",
        "\"ü§ó Transformers provides interoperability between which frameworks?\",\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "    outputs = model(**inputs)\n",
        "    answer_start_scores = outputs.start_logits\n",
        "    answer_end_scores = outputs.end_logits\n",
        "    answer_start = torch.argmax(\n",
        "        answer_start_scores\n",
        "    )  # Get the most likely beginning of answer with the argmax of the score\n",
        "    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "id": "sAy5cPClaFi2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbdLJZKqG2EbyeVTFIRdv9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}