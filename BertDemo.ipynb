{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1JXNPutxx6PZWSv+5Oxq/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changedi/DPpro/blob/master/BertDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "yfoNbGNppuC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "bt = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bt('I like natural language progressing!')\n",
        "# {'input_ids': [101, 1045, 2066, 3019, 2653, 27673, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n"
      ],
      "metadata": {
        "id": "oqwHqoMHqGB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BertForPreTraining"
      ],
      "metadata": {
        "id": "-HFJ06PFNuqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_CHECKPOINT_FOR_DOC = \"bert-base-uncased\"\n",
        "_CONFIG_FOR_DOC = \"BertConfig\"\n",
        "_TOKENIZER_FOR_DOC = \"BertTokenizer\"\n",
        "from transformers.models.bert.modeling_bert import *\n",
        "from transformers.models.bert.configuration_bert import *\n",
        "class BertForPreTraining(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.cls = BertPreTrainingHeads(config)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.cls.predictions.decoder\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.cls.predictions.decoder = new_embeddings\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @replace_return_docstrings(output_type=BertForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        next_sentence_label=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape ``(batch_size, sequence_length)``, `optional`):\n",
        "            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n",
        "            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n",
        "            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n",
        "        next_sentence_label (``torch.LongTensor`` of shape ``(batch_size,)``, `optional`):\n",
        "            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair\n",
        "            (see :obj:`input_ids` docstring) Indices should be in ``[0, 1]``:\n",
        "            - 0 indicates sequence B is a continuation of sequence A,\n",
        "            - 1 indicates sequence B is a random sequence.\n",
        "        kwargs (:obj:`Dict[str, any]`, optional, defaults to `{}`):\n",
        "            Used to hide legacy arguments that have been deprecated.\n",
        "        Returns:\n",
        "        Example::\n",
        "from transformers import BertTokenizer, BertForPreTraining\n",
        "import torch\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForPreTraining.from_pretrained('bert-base-uncased')\n",
        "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "prediction_logits = outputs.prediction_logits\n",
        "seq_relationship_logits = outputs.seq_relationship_logits\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output, pooled_output = outputs[:2]\n",
        "        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
        "\n",
        "        total_loss = None\n",
        "        if labels is not None and next_sentence_label is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
        "            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n",
        "            total_loss = masked_lm_loss + next_sentence_loss\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (prediction_scores, seq_relationship_score) + outputs[2:]\n",
        "            return ((total_loss,) + output) if total_loss is not None else output\n",
        "\n",
        "        return BertForPreTrainingOutput(\n",
        "            loss=total_loss,\n",
        "            prediction_logits=prediction_scores,\n",
        "            seq_relationship_logits=seq_relationship_score,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "from transformers import BertTokenizer, BertForPreTraining\n",
        "import torch\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForPreTraining.from_pretrained('bert-base-uncased')\n",
        "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "prediction_logits = outputs.prediction_logits\n",
        "seq_relationship_logits = outputs.seq_relationship_logits"
      ],
      "metadata": {
        "id": "mavI1zBWNAss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhKOro94NPOz",
        "outputId": "c206deec-3b5f-477f-ce4d-c70b89f46b6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForPreTrainingOutput(loss=None, prediction_logits=tensor([[[ -7.8962,  -7.8105,  -7.7903,  ...,  -7.0694,  -7.1693,  -4.3590],\n",
              "         [ -8.4461,  -8.4401,  -8.5044,  ...,  -8.0625,  -7.9909,  -5.7160],\n",
              "         [-15.2953, -15.4727, -15.5865,  ..., -12.9857, -11.7039, -11.4293],\n",
              "         ...,\n",
              "         [-14.0628, -14.2535, -14.3645,  ..., -12.7151, -11.1621, -10.2317],\n",
              "         [-10.6576, -10.7892, -11.0402,  ..., -10.3233, -10.1578,  -3.7722],\n",
              "         [-11.3383, -11.4590, -11.1767,  ...,  -9.2152,  -9.5209,  -9.5571]]],\n",
              "       grad_fn=<ViewBackward0>), seq_relationship_logits=tensor([[ 3.3474, -2.0613]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BertLMHeadModel：这个和上一个的区别在于，这一模型是作为 decoder 运行的版本；\n",
        "同样基于BertOnlyMLMHead；"
      ],
      "metadata": {
        "id": "AaOMJB2IfZTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@add_start_docstrings(\n",
        "    \"\"\"Bert Model with a `language modeling` head on top for CLM fine-tuning. \"\"\", BERT_START_DOCSTRING\n",
        ")\n",
        "class BertLMHeadModel(BertPreTrainedModel):\n",
        "\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        if not config.is_decoder:\n",
        "            logger.warning(\"If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\")\n",
        "\n",
        "        self.bert = BertModel(config, add_pooling_layer=False)\n",
        "        self.cls = BertOnlyMLMHead(config)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.cls.predictions.decoder\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.cls.predictions.decoder = new_embeddings\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        labels=None,\n",
        "        past_key_values=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
        "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
        "            the model is configured as a decoder.\n",
        "        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
        "            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n",
        "            ``[-100, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are\n",
        "            ignored (masked), the loss is only computed for the tokens with labels n ``[0, ..., config.vocab_size]``\n",
        "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
        "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
        "            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n",
        "            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n",
        "            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n",
        "        use_cache (:obj:`bool`, `optional`):\n",
        "            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n",
        "            decoding (see :obj:`past_key_values`).\n",
        "        Returns:\n",
        "        Example::\n",
        "            from transformers import BertTokenizer, BertLMHeadModel, BertConfig\n",
        "            import torch\n",
        "            tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "            config = BertConfig.from_pretrained(\"bert-base-cased\")\n",
        "            config.is_decoder = True\n",
        "            model = BertLMHeadModel.from_pretrained('bert-base-cased', config=config)\n",
        "            inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "            outputs = model(**inputs)\n",
        "            prediction_logits = outputs.logits\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        if labels is not None:\n",
        "            use_cache = False\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        prediction_scores = self.cls(sequence_output)\n",
        "\n",
        "        lm_loss = None\n",
        "        if labels is not None:\n",
        "            # we are doing next-token prediction; shift prediction scores and input ids by one\n",
        "            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n",
        "            labels = labels[:, 1:].contiguous()\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (prediction_scores,) + outputs[2:]\n",
        "            return ((lm_loss,) + output) if lm_loss is not None else output\n",
        "\n",
        "        return CausalLMOutputWithCrossAttentions(\n",
        "            loss=lm_loss,\n",
        "            logits=prediction_scores,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "            cross_attentions=outputs.cross_attentions,\n",
        "        )\n",
        "\n",
        "    def prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, **model_kwargs):\n",
        "        input_shape = input_ids.shape\n",
        "        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n",
        "        if attention_mask is None:\n",
        "            attention_mask = input_ids.new_ones(input_shape)\n",
        "\n",
        "        # cut decoder_input_ids if past is used\n",
        "        if past is not None:\n",
        "            input_ids = input_ids[:, -1:]\n",
        "\n",
        "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"past_key_values\": past}\n",
        "\n",
        "    def _reorder_cache(self, past, beam_idx):\n",
        "        reordered_past = ()\n",
        "        for layer_past in past:\n",
        "            reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n",
        "        return reordered_past\n",
        "\n",
        "from transformers import BertTokenizer, BertLMHeadModel, BertConfig\n",
        "import torch\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
        "config.is_decoder = True\n",
        "model = BertLMHeadModel.from_pretrained('bert-base-uncased', config=config)\n",
        "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "prediction_logits = outputs.logits"
      ],
      "metadata": {
        "id": "ASvMkR48N_jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs"
      ],
      "metadata": {
        "id": "AI75Vp3HelTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BertForNextSentencePrediction：只进行 NSP 任务的预训练。\n",
        "基于BertOnlyNSPHead，内容就是一个线性层。"
      ],
      "metadata": {
        "id": "HE5Ini9KfhL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertForNextSentencePrediction(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.cls = BertOnlyNSPHead(config)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @replace_return_docstrings(output_type=NextSentencePredictorOutput, config_class=_CONFIG_FOR_DOC)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair\n",
        "            (see ``input_ids`` docstring). Indices should be in ``[0, 1]``:\n",
        "            - 0 indicates sequence B is a continuation of sequence A,\n",
        "            - 1 indicates sequence B is a random sequence.\n",
        "        Returns:\n",
        "        Example::\n",
        "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
        "import torch\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
        "prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
        "next_sentence = \"The sky is blue due to the shorter wavelength of blue light.\"\n",
        "encoding = tokenizer(prompt, next_sentence, return_tensors='pt')\n",
        "outputs = model(**encoding, labels=torch.LongTensor([1]))\n",
        "logits = outputs.logits\n",
        "assert logits[0, 0] < logits[0, 1] # next sentence was random\n",
        "        \"\"\"\n",
        "\n",
        "        if \"next_sentence_label\" in kwargs:\n",
        "            warnings.warn(\n",
        "                \"The `next_sentence_label` argument is deprecated and will be removed in a future version, use `labels` instead.\",\n",
        "                FutureWarning,\n",
        "            )\n",
        "            labels = kwargs.pop(\"next_sentence_label\")\n",
        "\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        seq_relationship_scores = self.cls(pooled_output)\n",
        "\n",
        "        next_sentence_loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            next_sentence_loss = loss_fct(seq_relationship_scores.view(-1, 2), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (seq_relationship_scores,) + outputs[2:]\n",
        "            return ((next_sentence_loss,) + output) if next_sentence_loss is not None else output\n",
        "\n",
        "        return NextSentencePredictorOutput(\n",
        "            loss=next_sentence_loss,\n",
        "            logits=seq_relationship_scores,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
        "import torch\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
        "prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
        "next_sentence = \"The sky is blue due to the shorter wavelength of blue light.\"\n",
        "encoding = tokenizer(prompt, next_sentence, return_tensors='pt')\n",
        "outputs = model(**encoding, labels=torch.LongTensor([1]))\n",
        "logits = outputs.logits\n",
        "assert logits[0, 0] < logits[0, 1] # next sentence was random"
      ],
      "metadata": {
        "id": "BUs0dRq7e6DE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IY0uJu8Ke9tL",
        "outputId": "5ab84c1b-097c-4fa4-911d-d6cfbef42a64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NextSentencePredictorOutput(loss=tensor(0.0001, grad_fn=<NllLossBackward0>), logits=tensor([[-3.0729,  5.9056]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BertForSequenceClassification\n",
        "这一模型用于句子分类（也可以是回归）任务，比如 GLUE benchmark 的各个任务。\n",
        "\n",
        "句子分类的输入为句子（对），输出为单个分类标签。"
      ],
      "metadata": {
        "id": "83euq1DvNxry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled\n",
        "    output) e.g. for GLUE tasks.\n",
        "    \"\"\",\n",
        "    BERT_START_DOCSTRING,\n",
        ")\n",
        "class BertForSequenceClassification(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.config = config\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        classifier_dropout = (\n",
        "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
        "        )\n",
        "        self.dropout = nn.Dropout(classifier_dropout)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=SequenceClassifierOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
        "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
        "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.config.problem_type is None:\n",
        "                if self.num_labels == 1:\n",
        "                    self.config.problem_type = \"regression\"\n",
        "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
        "                    self.config.problem_type = \"single_label_classification\"\n",
        "                else:\n",
        "                    self.config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "            if self.config.problem_type == \"regression\":\n",
        "                loss_fct = MSELoss()\n",
        "                if self.num_labels == 1:\n",
        "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
        "                else:\n",
        "                    loss = loss_fct(logits, labels)\n",
        "            elif self.config.problem_type == \"single_label_classification\":\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            elif self.config.problem_type == \"multi_label_classification\":\n",
        "                loss_fct = BCEWithLogitsLoss()\n",
        "                loss = loss_fct(logits, labels)\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "VCZEG1rZk4sw",
        "outputId": "1ae2dcd8-44ee-4b4c-a4f5-a05f71d15707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-76f6f0081aea>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mBERT_START_DOCSTRING\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBertPreTrainedModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-76f6f0081aea>\u001b[0m in \u001b[0;36mBertForSequenceClassification\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0madd_start_docstrings_to_model_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBERT_INPUTS_DOCSTRING\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"batch_size, sequence_length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     @add_code_sample_docstrings(\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mtokenizer_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_TOKENIZER_FOR_DOC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_CHECKPOINT_FOR_DOC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: add_code_sample_docstrings() got an unexpected keyword argument 'tokenizer_class'"
          ]
        }
      ]
    }
  ]
}