{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPs91NP1JcioL9KggjbWp7/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changedi/DPpro/blob/master/aiops_ragbuild.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "-DrdOvwQY5jY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UbojSe1YnYd"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "\n",
        "from llama_index.core import Settings, QueryBundle\n",
        "from llama_index.core.base.llms.types import ChatMessage, MessageRole\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.legacy.llms import DashScope, DashScopeGenerationModels\n",
        "from qdrant_client import models\n",
        "from tqdm.asyncio import tqdm\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeding = HuggingFaceEmbedding(\n",
        "        model_name=\"BAAI/bge-large-zh-v1.5\",\n",
        "        cache_folder=\"./\",\n",
        "        embed_batch_size=128,\n",
        "    )\n",
        "Settings.embed_model = embeding"
      ],
      "metadata": {
        "id": "ec7GxZvMawRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core.embeddings import BaseEmbedding\n",
        "from llama_index.core.extractors import SummaryExtractor\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "from llama_index.core.llms.llm import LLM\n",
        "from llama_index.core.vector_stores.types import BasePydanticVectorStore\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.schema import Document, MetadataMode\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "from qdrant_client import AsyncQdrantClient, models\n",
        "from qdrant_client.http.exceptions import UnexpectedResponse"
      ],
      "metadata": {
        "id": "S13LO5-4bOdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Sequence\n",
        "from llama_index.core.extractors.interface import BaseExtractor\n",
        "from llama_index.core.schema import BaseNode\n",
        "import re\n",
        "\n",
        "class CustomFilePathExtractor(BaseExtractor):\n",
        "    last_path_length: int = 4\n",
        "\n",
        "    def __init__(self, last_path_length: int = 4, **kwargs):\n",
        "        super().__init__(last_path_length=last_path_length, **kwargs)\n",
        "\n",
        "    @classmethod\n",
        "    def class_name(cls) -> str:\n",
        "        return \"CustomFilePathExtractor\"\n",
        "\n",
        "    async def aextract(self, nodes: Sequence[BaseNode]) -> list[dict]:\n",
        "        metadata_list = []\n",
        "        for node in nodes:\n",
        "            node.metadata[\"file_path\"] = \"/\".join(\n",
        "                node.metadata[\"file_path\"].split(\"/\")[-self.last_path_length :]\n",
        "            )\n",
        "            if node.metadata[\"file_path\"] is not None:\n",
        "              rematch = re.search(r'data/(umac|director|rcp|emsplus)/',node.metadata[\"file_path\"])\n",
        "              if rematch is not None:\n",
        "                file_source = rematch.group(1)\n",
        "                if file_source is not None:\n",
        "                    node.metadata[\"file_source_dir\"] = file_source\n",
        "            metadata_list.append(node.metadata)\n",
        "        return metadata_list\n",
        "\n",
        "\n",
        "class CustomTitleExtractor(BaseExtractor):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    @classmethod\n",
        "    def class_name(cls) -> str:\n",
        "        return \"CustomTitleExtractor\"\n",
        "\n",
        "    # 将Document的第一行作为标题\n",
        "    async def aextract(self, nodes: Sequence[BaseNode]) -> list[dict]:\n",
        "        try:\n",
        "            document_title = nodes[0].text.split(\"\\n\")[0]\n",
        "            last_file_path = nodes[0].metadata[\"file_path\"]\n",
        "        except:\n",
        "            document_title = \"\"\n",
        "            last_file_path = \"\"\n",
        "        metadata_list = []\n",
        "        for node in nodes:\n",
        "            if node.metadata[\"file_path\"] != last_file_path:\n",
        "                document_title = node.text.split(\"\\n\")[0]\n",
        "                last_file_path = node.metadata[\"file_path\"]\n",
        "            node.metadata[\"document_title\"] = document_title\n",
        "            metadata_list.append(node.metadata)\n",
        "\n",
        "        return metadata_list\n"
      ],
      "metadata": {
        "id": "UWaz3sIZeheX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(path: str = \"data\") -> list[Document]:\n",
        "    reader = SimpleDirectoryReader(\n",
        "        input_dir=path,\n",
        "        recursive=True,\n",
        "        required_exts=[\n",
        "            \".txt\",\n",
        "        ],\n",
        "    )\n",
        "    return reader.load_data()\n",
        "\n",
        "def build_pipeline(\n",
        "    llm: LLM,\n",
        "    embed_model: BaseEmbedding,\n",
        "    template: str = None,\n",
        "    vector_store: BasePydanticVectorStore = None,\n",
        ") -> IngestionPipeline:\n",
        "    transformation = [\n",
        "        SentenceSplitter(chunk_size=1024, chunk_overlap=50),\n",
        "        CustomTitleExtractor(metadata_mode=MetadataMode.EMBED),\n",
        "        CustomFilePathExtractor(last_path_length=4, metadata_mode=MetadataMode.EMBED),\n",
        "        # SummaryExtractor(\n",
        "        #     llm=llm,\n",
        "        #     metadata_mode=MetadataMode.EMBED,\n",
        "        #     prompt_template=template or SUMMARY_EXTRACT_TEMPLATE,\n",
        "        # ),\n",
        "        embed_model,\n",
        "    ]\n",
        "\n",
        "    return IngestionPipeline(transformations=transformation, vector_store=vector_store)\n",
        "\n",
        "async def build_vector_store(\n",
        "    config: dict, reindex: bool = False\n",
        ") -> tuple[AsyncQdrantClient, QdrantVectorStore]:\n",
        "    client = AsyncQdrantClient(\n",
        "        # url=config[\"QDRANT_URL\"],\n",
        "        # location=\":memory:\",\n",
        "        path=\"./qdrant\"\n",
        "    )\n",
        "    if reindex:\n",
        "        try:\n",
        "            await client.delete_collection(config[\"COLLECTION_NAME\"] or \"aiops24\")\n",
        "            print(\"Reindex Start...\")\n",
        "        except UnexpectedResponse as e:\n",
        "            print(f\"Collection not found: {e}\")\n",
        "\n",
        "    try:\n",
        "        if not await client.collection_exists(config[\"COLLECTION_NAME\"]):\n",
        "            print(\"Collection Creating...\")\n",
        "            await client.create_collection(\n",
        "                collection_name=config[\"COLLECTION_NAME\"] or \"aiops24\",\n",
        "                on_disk_payload=True,\n",
        "                vectors_config=models.VectorParams(\n",
        "                    size=config[\"VECTOR_SIZE\"] or 1024, distance=models.Distance.DOT\n",
        "                ),\n",
        "            )\n",
        "            print(\"Collection Created!\")\n",
        "    except UnexpectedResponse:\n",
        "        print(\"Collection already exists\")\n",
        "    return client, QdrantVectorStore(\n",
        "        aclient=client,\n",
        "        collection_name=config[\"COLLECTION_NAME\"] or \"aiops24\",\n",
        "        parallel=4,\n",
        "        batch_size=32,\n",
        "    )"
      ],
      "metadata": {
        "id": "aPI8DOurbA9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"COLLECTION_NAME\":\"aiops24\", \"VECTOR_SIZE\":1024}\n",
        "client, vector_store = await build_vector_store(config, reindex=True)\n",
        "\n",
        "collection_info = await client.get_collection(\n",
        "  config[\"COLLECTION_NAME\"] or \"aiops24\"\n",
        ")"
      ],
      "metadata": {
        "id": "0sJlYp-mbghU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Iterable\n",
        "import jsonlines\n",
        "\n",
        "\n",
        "def read_jsonl(path):\n",
        "    content = []\n",
        "    with jsonlines.open(path, \"r\") as json_file:\n",
        "        for obj in json_file.iter(type=dict, skip_invalid=True):\n",
        "            content.append(obj)\n",
        "    return content\n",
        "\n",
        "\n",
        "def save_answers(\n",
        "    queries: Iterable, results: Iterable, path: str = \"data/answers.jsonl\"\n",
        "):\n",
        "    answers = []\n",
        "    for query, result in zip(queries, results):\n",
        "        answers.append(\n",
        "            {\"id\": query[\"id\"], \"query\": query[\"query\"], \"answer\": result}\n",
        "        )\n",
        "\n",
        "    # use jsonlines to save the answers\n",
        "    def write_jsonl(path, content):\n",
        "        with jsonlines.open(path, \"w\") as json_file:\n",
        "            json_file.write_all(content)\n",
        "\n",
        "    # 保存答案到 data/answers.jsonl\n",
        "    write_jsonl(path, answers)\n"
      ],
      "metadata": {
        "id": "MPw80pZwdPqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://www.modelscope.cn/datasets/issaccv/aiops2024-challenge-dataset.git /dataset"
      ],
      "metadata": {
        "id": "LGNKv5rMdSHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /dataset/data.zip -d ./"
      ],
      "metadata": {
        "id": "ellmvb4Sdk-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import dotenv_values, load_dotenv\n",
        "load_dotenv()\n",
        "os.environ[\"DASHSCOPE_API_KEY\"] = os.getenv(\"DASHSCOPE_API_KEY\")\n",
        "llm = DashScope(model_name=DashScopeGenerationModels.QWEN_TURBO)"
      ],
      "metadata": {
        "id": "iYcFSq3Ye2hE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ff = re.search(r'data/(umac|director|rcp|emsplus)/',\"abcdafds/fdsfsdfs/fdsfdsf/data/umac/fdsafew\").group(1)\n",
        "print(ff)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nqXtA_GiERQ",
        "outputId": "026d700d-3ae3-40f2-e76d-0c1bda81f6e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "umac\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = read_data(\"data\")"
      ],
      "metadata": {
        "id": "BzwxgGzrjTrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if collection_info.points_count == 0:\n",
        "        t_start = time.time()\n",
        "        print(f\"read data finished in {time.time()-t_start}s and total {len(data)} records.\")\n",
        "        pipeline = build_pipeline(llm, embeding, vector_store=vector_store)\n",
        "        # 暂时停止实时索引\n",
        "        await client.update_collection(\n",
        "            collection_name=config[\"COLLECTION_NAME\"] or \"aiops24\",\n",
        "            optimizer_config=models.OptimizersConfigDiff(indexing_threshold=0),\n",
        "        )\n",
        "        t_start = time.time()\n",
        "        await pipeline.arun(documents=data, show_progress=True, num_workers=1)\n",
        "        # 恢复实时索引\n",
        "        await client.update_collection(\n",
        "            collection_name=config[\"COLLECTION_NAME\"] or \"aiops24\",\n",
        "            optimizer_config=models.OptimizersConfigDiff(indexing_threshold=20000),\n",
        "        )\n",
        "        print(f\"build vector store finished. in {time.time()-t_start}s, total:{collection_info.points_count}\")"
      ],
      "metadata": {
        "id": "FStAVU-PdGYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://www.modelscope.cn/api/v1/models/qwen/Qwen2-7B-Instruct-GGUF/repo?Revision=master&FilePath=qwen2-7b-instruct-q8_0.gguf\" -O qwen-2-7b-instruct.gguf"
      ],
      "metadata": {
        "id": "ri6ZxNFHyUZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "BEatX9Xf3F1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!OLLAMA_FLASH_ATTENTION=1 nohup /usr/local/bin/ollama serve &"
      ],
      "metadata": {
        "id": "igu23tVe3VFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"FROM ./qwen-2-7b-instruct.gguf\" > Model_file\n",
        "!/usr/local/bin/ollama create qwen_local -f Model_file\n"
      ],
      "metadata": {
        "id": "XdWkLf2V0kvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/bin/ollama ps"
      ],
      "metadata": {
        "id": "n6BMCBxdAcjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup /usr/local/bin/ollama run qwen_local &"
      ],
      "metadata": {
        "id": "nqWbZ8aeADsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl http://localhost:11434/api/generate -d '{\"model\": \"qwen_local\",\"prompt\":\"Why is the sky blue?\"}'"
      ],
      "metadata": {
        "id": "A8IOfRZm1wG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.ollama import Ollama\n",
        "llm = Ollama(\n",
        "        model=\"qwen2\", base_url=\"http://localhost:11434\", temperature=0, request_timeout=120\n",
        "    )\n",
        "\n",
        "res = await llm.acomplete(\"你好，你是谁\")\n",
        "print(res)"
      ],
      "metadata": {
        "id": "zU-Raidq0zyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "import qdrant_client\n",
        "\n",
        "from llama_index.core.llms.llm import LLM\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "from llama_index.core.postprocessor.types import BaseNodePostprocessor\n",
        "from llama_index.core.vector_stores import VectorStoreQuery, MetadataFilter, FilterOperator, MetadataFilters\n",
        "from llama_index.core import (\n",
        "    QueryBundle,\n",
        "    PromptTemplate,\n",
        "    StorageContext,\n",
        "    VectorStoreIndex,\n",
        ")\n",
        "from llama_index.core.embeddings import BaseEmbedding\n",
        "from llama_index.core.retrievers import BaseRetriever\n",
        "from llama_index.core.schema import NodeWithScore\n",
        "from llama_index.core.base.llms.types import CompletionResponse, ChatMessage, MessageRole\n",
        "\n",
        "from qdrant_client.http.models import Filter, FieldCondition, MatchValue, MatchAny\n",
        "\n",
        "\n",
        "QA_TEMPLATE = \"\"\"\\\n",
        "    上下文信息如下：\n",
        "    ----------\n",
        "    {context_str}\n",
        "    ----------\n",
        "    请你基于上下文信息而不是自己的知识，回答以下问题，可以分点作答，如果上下文信息没有相关知识，你可以尝试自己回答，不要复述上下文信息，不要输出你不知道的知识.\n",
        "    请注意区分问题类型，如果问题是问为什么，请解答原因；如果问题是问如何做，请列出方法和步骤；如果问题是问是什么，请直接回答是什么.\n",
        "    问题：\\\n",
        "    {query_str}\n",
        "\n",
        "    回答：\\\n",
        "    \"\"\"\n",
        "\n",
        "class QdrantRetriever(BaseRetriever):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vector_store: QdrantVectorStore,\n",
        "        embed_model: BaseEmbedding,\n",
        "        similarity_top_k: int = 2,\n",
        "    ) -> None:\n",
        "        self._vector_store = vector_store\n",
        "        self._embed_model = embed_model\n",
        "        self._similarity_top_k = similarity_top_k\n",
        "        super().__init__()\n",
        "\n",
        "    async def _aretrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
        "        query_embedding = self._embed_model.get_query_embedding(query_bundle.query_str)\n",
        "        vector_store_query = VectorStoreQuery(\n",
        "            query_embedding, similarity_top_k=self._similarity_top_k\n",
        "        )\n",
        "        filters = Filter(\n",
        "            should=[\n",
        "                Filter(\n",
        "                    must=[\n",
        "                        FieldCondition(\n",
        "                            key=\"file_source_dir\",\n",
        "                            match=MatchValue(value=query_bundle.custom_embedding_strs[0]),\n",
        "                        )\n",
        "                    ]\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "        query_result = await self._vector_store.aquery(vector_store_query, qdrant_filters=filters)\n",
        "\n",
        "        node_with_scores = []\n",
        "        for node, similarity in zip(query_result.nodes, query_result.similarities):\n",
        "            if len(node_with_scores) == 0:\n",
        "                node_with_scores.append(NodeWithScore(node=node, score=similarity))\n",
        "            elif similarity >= 0.3:\n",
        "                node_with_scores.append(NodeWithScore(node=node, score=similarity))\n",
        "        return node_with_scores\n",
        "\n",
        "\n",
        "async def rag_with_knowledge_retrieval_querybundle(\n",
        "    query_bundle: QueryBundle,\n",
        "    retriever: BaseRetriever,\n",
        "    llm: LLM,\n",
        "    qa_template: str = QA_TEMPLATE,\n",
        "    reranker: BaseNodePostprocessor | None = None,\n",
        "    debug: bool = False,\n",
        "    detail: bool = False,\n",
        "    generate: bool = True,\n",
        "    progress=None,\n",
        ") -> str:\n",
        "    node_with_scores = await retriever.aretrieve(query_bundle)\n",
        "    if debug:\n",
        "        score_str = \",\".join(\n",
        "            [f\"{node.metadata['document_title']}: {node.score}\" for node in node_with_scores]\n",
        "        )\n",
        "        print(f\"retrieved question: {query_bundle.query_str} \\nnodes num:{len(node_with_scores)} \\nscores:{score_str}\\n------\")\n",
        "    if detail and debug:\n",
        "        print(f\"retrieved {query_bundle.query_str}:\\n{node_with_scores}\\n------\")\n",
        "    if reranker:\n",
        "        node_with_scores = reranker.postprocess_nodes(node_with_scores, query_bundle)\n",
        "        if debug:\n",
        "            print(f\"reranked:\\n{node_with_scores}\\n------\")\n",
        "    context_str = \"\\n\\n\".join(\n",
        "        [f\"{node.metadata['document_title']}: {node.text}\" for node in node_with_scores]\n",
        "    )\n",
        "    fmt_qa_prompt = PromptTemplate(qa_template).format(\n",
        "        context_str=context_str, query_str=query_bundle.query_str\n",
        "    )\n",
        "    ret = None\n",
        "    if generate:\n",
        "        user_msg = ChatMessage.from_str(fmt_qa_prompt, MessageRole.USER)\n",
        "        sys_msg = ChatMessage.from_str(\n",
        "            \"你是一个云计算和网络领域的运维工程师，请尽量识别领域关键词和关键信息，阅读给定的知识回答问题\", MessageRole.SYSTEM)\n",
        "        chat_response = await llm.achat([sys_msg, user_msg])\n",
        "        ret = chat_response.message.content\n",
        "        if debug:\n",
        "            print(f\"generated :\\n{ret}\\n\")\n",
        "    if progress:\n",
        "        progress.update(1)\n",
        "    return ret\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "ptryqFzLtSTb",
        "outputId": "e41aa33b-fe15-457e-82f7-dc14d8a6cc87"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'qdrant_client'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-234e95e70265>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mqdrant_client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_stores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqdrant\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQdrantVectorStore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'qdrant_client'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = QdrantRetriever(vector_store, embeding, similarity_top_k=5)\n",
        "\n",
        "# query_bundle = QueryBundle(query_str=\"\"\"\n",
        "#         SGSN/MME网元的采集方式有哪几种？\n",
        "#     \"\"\", custom_embedding_strs=[\"umac\"])\n",
        "# result = await rag_with_knowledge_retrieval_querybundle(\n",
        "#     query_bundle, retriever, llm, debug=True, detail=True\n",
        "# )\n",
        "# print(result)\n",
        "\n",
        "# 检查检索结果\n",
        "# queries = read_jsonl(\"question.jsonl\")\n",
        "# for query in tqdm(queries, total=len(queries)):\n",
        "#     query_bundle = QueryBundle(query_str=query[\"query\"], custom_embedding_strs=[query[\"document\"]])\n",
        "#     await rag_with_knowledge_retrieval_querybundle(\n",
        "#         query_bundle, retriever, llm, debug=True, generate=False\n",
        "#     )\n",
        "\n",
        "# 生成答案\n",
        "print(\"Start generating answers...\")\n",
        "queries = read_jsonl(\"question.jsonl\")\n",
        "results = []\n",
        "for query in tqdm(queries, total=len(queries)):\n",
        "    # if query[\"id\"] > 2:\n",
        "    #     break\n",
        "    query_bundle = QueryBundle(query_str=query[\"query\"], custom_embedding_strs=[query[\"document\"]])\n",
        "    result = await rag_with_knowledge_retrieval_querybundle(\n",
        "        query_bundle, retriever, llm, debug=True\n",
        "    )\n",
        "    results.append(result)\n",
        "\n",
        "# 处理结果\n",
        "save_answers(queries, results, \"result.jsonl\")"
      ],
      "metadata": {
        "id": "zBa04CbtuX_M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}