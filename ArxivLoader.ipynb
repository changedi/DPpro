{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOueG8PXC+JYMG8KcSOBiSn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changedi/DPpro/blob/master/ArxivLoader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOmqqT1FiVKe"
      },
      "outputs": [],
      "source": [
        "!pip install langchain --upgrade\n",
        "# Version: 0.0.164\n",
        "\n",
        "!pip install pypdf\n",
        "\n",
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PDF Loaders. If unstructured gives you a hard time, try PyPDFLoader\n",
        "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import os"
      ],
      "metadata": {
        "id": "utXk946cjE-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arxiv"
      ],
      "metadata": {
        "id": "0XUkUHKRlncv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsVPVCbIl5FE",
        "outputId": "4974493d-3f34-4d78-fcba-a42277209436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.23.3-cp310-none-manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.23.3 (from pymupdf)\n",
            "  Downloading PyMuPDFb-1.23.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, pymupdf\n",
            "Successfully installed PyMuPDFb-1.23.3 pymupdf-1.23.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import ArxivRetriever\n",
        "retriever = ArxivRetriever(load_max_docs=2)\n",
        "docs = retriever.get_relevant_documents(query=\"2208.13629\")"
      ],
      "metadata": {
        "id": "6rLp9_wMlrc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0].metadata  # meta-information of the Document"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DvwCWaQmBMc",
        "outputId": "7986f380-f9d5-4b79-e992-6f457087fa5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Published': '2022-08-29',\n",
              " 'Title': 'A Survey on Text-to-SQL Parsing: Concepts, Methods, and Future Directions',\n",
              " 'Authors': 'Bowen Qin, Binyuan Hui, Lihan Wang, Min Yang, Jinyang Li, Binhua Li, Ruiying Geng, Rongyu Cao, Jian Sun, Luo Si, Fei Huang, Yongbin Li',\n",
              " 'Summary': 'Text-to-SQL parsing is an essential and challenging task. The goal of\\ntext-to-SQL parsing is to convert a natural language (NL) question to its\\ncorresponding structured query language (SQL) based on the evidences provided\\nby relational databases. Early text-to-SQL parsing systems from the database\\ncommunity achieved a noticeable progress with the cost of heavy human\\nengineering and user interactions with the systems. In recent years, deep\\nneural networks have significantly advanced this task by neural generation\\nmodels, which automatically learn a mapping function from an input NL question\\nto an output SQL query. Subsequently, the large pre-trained language models\\nhave taken the state-of-the-art of the text-to-SQL parsing task to a new level.\\nIn this survey, we present a comprehensive review on deep learning approaches\\nfor text-to-SQL parsing. First, we introduce the text-to-SQL parsing corpora\\nwhich can be categorized as single-turn and multi-turn. Second, we provide a\\nsystematical overview of pre-trained language models and existing methods for\\ntext-to-SQL parsing. Third, we present readers with the challenges faced by\\ntext-to-SQL parsing and explore some potential future directions in this field.'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0].page_content[:400]  # a content of the Document"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "DmYuiP9VmEN6",
        "outputId": "5c7badb2-653d-4d26-ab90-f61c7405cdd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING\\n1\\nA Survey on Text-to-SQL Parsing: Concepts,\\nMethods, and Future Directions\\nBowen Qin, Binyuan Hui, Lihan Wang, Min Yang, Jinyang Li, Binhua Li, Ruiying Geng, Rongyu Cao, Jian\\nSun, Luo Si, Fei Huang, Yongbin Li\\nAbstract—Text-to-SQL parsing is an essential and challenging task. The goal of text-to-SQL parsing is to convert a natural language\\n(NL) '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "import os\n",
        "from langchain.llms import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "# for LangChain\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "pbUB8OXGjHFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Zv6bOSonC3N",
        "outputId": "7e24b8f2-76a6-469e-e225-2d790ef05dea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/76.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "model = ChatOpenAI(model_name=\"gpt-3.5-turbo\")  # switch to 'gpt-4'\n",
        "qa = ConversationalRetrievalChain.from_llm(model, retriever=retriever)"
      ],
      "metadata": {
        "id": "4AiPv752m8Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"What is Text-to-Sql?\",\n",
        "    \"What is the main idea of this paper?\",\n",
        "    \"What method does this paper provide?\",\n",
        "]\n",
        "chat_history = []\n",
        "\n",
        "for question in questions:\n",
        "    result = qa({\"question\": question, \"chat_history\": chat_history})\n",
        "    chat_history.append((question, result[\"answer\"]))\n",
        "    print(f\"-> **Question**: {question} \\n\")\n",
        "    print(f\"**Answer**: {result['answer']} \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDnJjzGKnLhc",
        "outputId": "f94ad18f-4707-47de-e544-326f4f709054"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-> **Question**: What is Text-to-Sql? \n",
            "\n",
            "**Answer**: I don't know what \"Text-to-Sql\" refers to. \n",
            "\n",
            "-> **Question**: What is the main idea of this paper? \n",
            "\n",
            "**Answer**: The main idea of this paper is to analyze a bifurcation phenomenon in the theory of noise-activated transitions in double-well systems. The authors study the behavior of the most probable transition path between the two wells as a system parameter is varied. They demonstrate that at the bifurcation point, the activation kinetics of the system become non-Arrhenius, meaning that the growth of the mean time between inter-well fluctuations is not purely exponential. The authors propose a new scaling theory to explain the weak-noise behavior at the bifurcation point and show that it applies to a large universality class of double-well systems. \n",
            "\n",
            "-> **Question**: What method does this paper provide? \n",
            "\n",
            "**Answer**: This paper provides a method for systematically deriving a finite set of observables to construct a lifted linear state space model that describes the rigid body dynamics based on the dual quaternion representation. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OPENAI的api调用\n"
      ],
      "metadata": {
        "id": "bzwBKbblwXhg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ChatCompletion"
      ],
      "metadata": {
        "id": "_507Ig2oxWWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of an OpenAI ChatCompletion request\n",
        "# https://platform.openai.com/docs/guides/chat\n",
        "import time\n",
        "import openai\n",
        "# record the time before the request is sent\n",
        "start_time = time.time()\n",
        "\n",
        "# send a ChatCompletion request to count to 100\n",
        "response = openai.ChatCompletion.create(\n",
        "    model='gpt-3.5-turbo',\n",
        "    messages=[\n",
        "        { 'role': 'user',\n",
        "         'content': 'Count to 10, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'\n",
        "         }\n",
        "    ],\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "# calculate the time it took to receive the response\n",
        "response_time = time.time() - start_time\n",
        "\n",
        "# print the time delay and text received\n",
        "print(f\"Full response received {response_time:.2f} seconds after request\")\n",
        "print(f\"Full response received:\\n{response}\")"
      ],
      "metadata": {
        "id": "a8DAnNCswglG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PromptCompletion"
      ],
      "metadata": {
        "id": "aYViW0Ynxh57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source = \"A suffix between a prompt and a completion is necessary to tell the model that the input text has stopped, and that it now needs to predict the class. Since we use the same separator in each example, the model is able to learn that it is meant to predict either baseball or hockey following the separator. A whitespace prefix in completions is useful, as most word tokens are tokenized with a space prefix. The tool also recognized that this is likely a classification task, so it suggested to split the dataset into training and validation datasets. This will allow us to easily measure expected performance on new data.\"\n",
        "prompt = f'''You are a helpful translater.\n",
        "Translate this sentence {source} to Chinese.\n",
        "'''\n",
        "response = openai.Completion.create(\n",
        "        prompt=prompt,\n",
        "        engine='text-davinci-002',\n",
        "        temperature=0,\n",
        "        top_p=1,\n",
        "        max_tokens=1500,\n",
        "    )\n",
        "\n",
        "print(f\"Full response received:\\n{response}\")\n",
        "print(response['choices'][0]['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MIukyTcxlzp",
        "outputId": "74ec5f82-06f8-4e8c-8b98-dde430de10c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full response received:\n",
            "{\n",
            "  \"warning\": \"This model version is deprecated. Migrate before January 4, 2024 to avoid disruption of service. Learn more https://platform.openai.com/docs/deprecations\",\n",
            "  \"id\": \"cmpl-80o7i2IcpIQHe0Z5pQnQU6oQ0UkOf\",\n",
            "  \"object\": \"text_completion\",\n",
            "  \"created\": 1695203366,\n",
            "  \"model\": \"text-davinci-002\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"text\": \"\\n\\u4e00\\u4e2a\\u540e\\u7f00\\u5728\\u63d0\\u793a\\u548c\\u5b8c\\u6210\\u4e4b\\u95f4\\u662f\\u5fc5\\u8981\\u7684\\uff0c\\u544a\\u8bc9\\u6a21\\u578b\\u8f93\\u5165\\u6587\\u672c\\u5df2\\u7ecf\\u505c\\u6b62\\uff0c\\u73b0\\u5728\\u9700\\u8981\\u9884\\u6d4b\\u7c7b\\u522b\\u3002\\u7531\\u4e8e\\u6211\\u4eec\\u5728\\u6bcf\\u4e2a\\u793a\\u4f8b\\u4e2d\\u4f7f\\u7528\\u76f8\\u540c\\u7684\\u5206\\u9694\\u7b26\\uff0c\\u6a21\\u578b\\u80fd\\u591f\\u5b66\\u4e60\\u5230\\u5b83\\u9700\\u8981\\u5728\\u5206\\u9694\\u7b26\\u4e4b\\u540e\\u9884\\u6d4b\\u68d2\\u7403\\u6216\\u51b0\\u7403\\u3002\\u5728\\u5b8c\\u6210\\u4e2d\\u4f7f\\u7528\\u7a7a\\u683c\\u524d\\u7f00\\u662f\\u6709\\u7528\\u7684\\uff0c\\u56e0\\u4e3a\\u5927\\u591a\\u6570\\u5355\\u8bcd\\u4ee4\\u724c\\u90fd\\u662f\\u5e26\\u6709\\u7a7a\\u683c\\u524d\\u7f00\\u7684\\u4ee4\\u724c\\u5316\\u3002\\u8be5\\u5de5\\u5177\\u8fd8\\u8ba4\\u4e3a\\u8fd9\\u53ef\\u80fd\\u662f\\u4e00\\u4e2a\\u5206\\u7c7b\\u4efb\\u52a1\\uff0c\\u56e0\\u6b64\\u5efa\\u8bae\\u5c06\\u6570\\u636e\\u96c6\\u5206\\u6210\\u8bad\\u7ec3\\u96c6\\u548c\\u9a8c\\u8bc1\\u96c6\\u3002\\u8fd9\\u5c06\\u4f7f\\u6211\\u4eec\\u80fd\\u591f\\u8f7b\\u677e\\u5730\\u6d4b\\u91cf\\u65b0\\u6570\\u636e\\u7684\\u9884\\u671f\\u6027\\u80fd\\u3002\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"finish_reason\": \"stop\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 140,\n",
            "    \"completion_tokens\": 369,\n",
            "    \"total_tokens\": 509\n",
            "  }\n",
            "}\n",
            "\n",
            "一个后缀在提示和完成之间是必要的，告诉模型输入文本已经停止，现在需要预测类别。由于我们在每个示例中使用相同的分隔符，模型能够学习到它需要在分隔符之后预测棒球或冰球。在完成中使用空格前缀是有用的，因为大多数单词令牌都是带有空格前缀的令牌化。该工具还认为这可能是一个分类任务，因此建议将数据集分成训练集和验证集。这将使我们能够轻松地测量新数据的预期性能。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding"
      ],
      "metadata": {
        "id": "Jf4IWPNS0SQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tenacity import retry, wait_random_exponential, stop_after_attempt, retry_if_not_exception_type\n",
        "EMBEDDING_MODEL = 'text-embedding-ada-002'\n",
        "EMBEDDING_CTX_LENGTH = 8191\n",
        "EMBEDDING_ENCODING = 'cl100k_base'\n",
        "\n",
        "# let's make sure to not retry on an invalid request, because that is what we want to demonstrate\n",
        "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6), retry=retry_if_not_exception_type(openai.InvalidRequestError))\n",
        "def get_embedding(text_or_tokens, model=EMBEDDING_MODEL):\n",
        "    return openai.Embedding.create(input=text_or_tokens, model=model)[\"data\"][0][\"embedding\"]\n",
        "\n",
        "long_text = 'AGI ' * 2000\n",
        "try:\n",
        "    embedding=get_embedding(long_text)\n",
        "    print(len(embedding))\n",
        "    print(embedding[:10])\n",
        "except openai.InvalidRequestError as e:\n",
        "    print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pKdREnJ0VLm",
        "outputId": "1d78a049-39a6-43ed-c1fd-50cfc2e44647"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1536\n",
            "[-0.013576747849583626, 0.005601828917860985, -0.008355950005352497, -0.016150379553437233, -0.00665802089497447, 0.0058759041130542755, -0.017634397372603416, 0.00013150175800547004, -0.005264248698949814, -0.024145353585481644]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model list"
      ],
      "metadata": {
        "id": "-6iO4xg059e7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelList = openai.Model.list()\n",
        "modelTypes = set()\n",
        "for model in modelList['data']:\n",
        "  modelTypes.add(model['owned_by'])\n",
        "print(modelTypes)\n",
        "\n",
        "for tp in modelTypes:\n",
        "  print(f\"模型类型:{tp}\")\n",
        "  for model in modelList['data']:\n",
        "    if(model['owned_by']==tp and model['object']=='model'):\n",
        "      print(f\"\\t{model['id']}\")"
      ],
      "metadata": {
        "id": "PLR75Boa5Oxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DALL-E image"
      ],
      "metadata": {
        "id": "vKj_DEhL__kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "# set a directory to save DALL·E images to\n",
        "image_dir_name = \"images\"\n",
        "image_dir = os.path.join(os.curdir, image_dir_name)\n",
        "\n",
        "# create the directory if it doesn't yet exist\n",
        "if not os.path.isdir(image_dir):\n",
        "    os.mkdir(image_dir)\n",
        "\n",
        "# print the directory to save to\n",
        "print(f\"{image_dir=}\")\n",
        "# create an image\n",
        "\n",
        "# set the prompt\n",
        "# prompt = \"A cyberpunk monkey hacker dreaming of a beautiful bunch of bananas, digital art\"\n",
        "prompt = \"a boy, Happy, Chest shot, \"\n",
        "\n",
        "# call the OpenAI API\n",
        "generation_response = openai.Image.create(\n",
        "    prompt=prompt,\n",
        "    n=1,\n",
        "    size=\"512x512\",\n",
        "    response_format=\"url\",\n",
        ")\n",
        "\n",
        "# print response\n",
        "print(generation_response)\n",
        "# save the image\n",
        "generated_image_name = \"generated_image.png\"  # any name you like; the filetype should be .png\n",
        "generated_image_filepath = os.path.join(image_dir, generated_image_name)\n",
        "generated_image_url = generation_response[\"data\"][0][\"url\"]  # extract image URL from response\n",
        "generated_image = requests.get(generated_image_url).content  # download the image\n",
        "\n",
        "with open(generated_image_filepath, \"wb\") as image_file:\n",
        "    image_file.write(generated_image)  # write the image to the file\n",
        "display(Image.open(generated_image_filepath))"
      ],
      "metadata": {
        "id": "ixTh8Vd4ACiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune"
      ],
      "metadata": {
        "id": "kztP7QIP1D1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/openai/openai-cookbook/raw/main/examples/data/cookbook_recipes_nlg_10k.csv"
      ],
      "metadata": {
        "id": "980aIa7g1Gi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/openai/openai-cookbook/raw/main/examples/data/fine_food_reviews_1k.csv"
      ],
      "metadata": {
        "id": "u0OeVmRT2KvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure to use the latest version of the openai python package\n",
        "!pip install --upgrade openai\n",
        "!pip install python-dotenv"
      ],
      "metadata": {
        "id": "EZnnGec32VNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import openai\n",
        "import os\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "np0ovRKu2Z-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in the dataset we'll use for this task.\n",
        "recipe_df = pd.read_csv(\"cookbook_recipes_nlg_10k.csv\")\n",
        "review_df = pd.read_csv(\"fine_food_reviews_1k.csv\")\n",
        "\n",
        "recipe_df.head()\n"
      ],
      "metadata": {
        "id": "trEPqOUU3Pjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_df.head()"
      ],
      "metadata": {
        "id": "fNBBpEE53elF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(recipe_df.size)\n",
        "print(review_df.size)\n"
      ],
      "metadata": {
        "id": "znoTTM0x71Z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preparing\n",
        "We'll begin by preparing our data. When fine-tuning with the ChatCompletion format, each training example is a simple list of messages. For example, an entry could look like:\n",
        "\n"
      ],
      "metadata": {
        "id": "Fw6yRW9r3iS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = []\n",
        "\n",
        "system_message_recipe = \"You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.\"\n",
        "\n",
        "system_message_review = \"I want you to act as a helpful reviewer. You are to summarise the main idea from each of the text provided and give it a score with 1 to 5.\"\n",
        "\n",
        "def create_user_message_recipe(row):\n",
        "    return f\"\"\"Title: {row['title']}\\n\\nIngredients: {row['ingredients']}\\n\\nGeneric ingredients: \"\"\"\n",
        "\n",
        "def create_user_message_review(row):\n",
        "    return f\"\"\"Text: {row['Text']}\\n##\\nPlease summarise and score: \"\"\"\n",
        "\n",
        "def prepare_example_conversation_recipe(row):\n",
        "    messages = []\n",
        "    messages.append({\"role\": \"system\", \"content\": system_message_recipe})\n",
        "\n",
        "    user_message = create_user_message_recipe(row)\n",
        "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    messages.append({\"role\": \"assistant\", \"content\": row[\"NER\"]})\n",
        "\n",
        "    return {\"messages\": messages}\n",
        "\n",
        "def prepare_example_conversation_review(row):\n",
        "    messages = []\n",
        "    messages.append({\"role\": \"system\", \"content\": system_message_review})\n",
        "\n",
        "    user_message = create_user_message_review(row)\n",
        "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    messages.append({\"role\": \"assistant\", \"content\": f\"\"\"Summary: {row['Summary']}\\n\\nScore: {row['Score']}\"\"\"})\n",
        "\n",
        "    return {\"messages\": messages}\n",
        "\n",
        "pprint(prepare_example_conversation_recipe(recipe_df.iloc[0]))\n",
        "pprint(prepare_example_conversation_review(review_df.iloc[0]))"
      ],
      "metadata": {
        "id": "61tis3Yn3hzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the first rows of the dataset for training\n",
        "training_df_recipe = recipe_df.loc[0:4999]\n",
        "training_df_review = review_df.loc[0:499]\n"
      ],
      "metadata": {
        "id": "7vgttMvf7ikt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the prepare_example_conversation function to each row of the training_df\n",
        "training_data = training_df_recipe.apply(prepare_example_conversation_recipe, axis=1).tolist() + training_df_review.apply(prepare_example_conversation_review, axis=1).tolist()\n",
        "\n",
        "len(training_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NW-kGrK78Wxz",
        "outputId": "515a9904-3e37-4b65-f8a9-09fd53358ff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5500"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_df_recipe = recipe_df.loc[5000:]\n",
        "validation_df_review = review_df.loc[500:]\n",
        "validation_data = validation_df_recipe.apply(prepare_example_conversation_recipe, axis=1).tolist() + validation_df_review.apply(prepare_example_conversation_review, axis=1).tolist()\n",
        "\n",
        "len(training_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0Ymymk699gF",
        "outputId": "6f40a259-8e6f-41d0-df1e-43a0cbda3e88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5500"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def write_jsonl(data_list: list, filename: str) -> None:\n",
        "    with open(filename, \"w\") as out:\n",
        "        for ddict in data_list:\n",
        "            jout = json.dumps(ddict) + \"\\n\"\n",
        "            out.write(jout)\n",
        "\n",
        "training_file_name = \"tmp_2r_finetune_training.jsonl\"\n",
        "write_jsonl(training_data, training_file_name)\n",
        "\n",
        "validation_file_name = \"tmp_2r_finetune_validation.jsonl\"\n",
        "write_jsonl(validation_data, validation_file_name)"
      ],
      "metadata": {
        "id": "X2FGN_bW-WQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 tmp_2r_finetune_training.jsonl"
      ],
      "metadata": {
        "id": "ymOzTG7g-0MM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 tmp_2r_finetune_validation.jsonl"
      ],
      "metadata": {
        "id": "PdMiA0eV-6E7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = OPENAI_API_KEY\n",
        "training_response = openai.File.create(\n",
        "    file=open(training_file_name, \"rb\"), purpose=\"fine-tune\"\n",
        ")\n",
        "training_file_id = training_response[\"id\"]\n",
        "\n",
        "validation_response = openai.File.create(\n",
        "    file=open(validation_file_name, \"rb\"), purpose=\"fine-tune\"\n",
        ")\n",
        "validation_file_id = validation_response[\"id\"]\n",
        "\n",
        "print(\"Training file ID:\", training_file_id)\n",
        "print(\"Validation file ID:\", validation_file_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vr-compS-9Fj",
        "outputId": "138337a6-88ed-48fc-bca2-352382dbe87b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training file ID: file-AY05c6lDECwrJLJenEfOq4O7\n",
            "Validation file ID: file-ANZ8AoH4U3vrLpncdte21Ca2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can create our fine-tuning job with the generated files and an optional suffix to identify the model. The response will contain an id which you can use to retrieve updates on the job."
      ],
      "metadata": {
        "id": "CCVzuCOE_xC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.FineTuningJob.create(\n",
        "    training_file=training_file_id,\n",
        "    validation_file=validation_file_id,\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    suffix=\"recipe-ner\",\n",
        ")\n",
        "\n",
        "job_id = response[\"id\"]\n",
        "\n",
        "print(\"Job ID:\", response[\"id\"])\n",
        "print(\"Status:\", response[\"status\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSTeK463_zq8",
        "outputId": "c7cb439c-0898-4878-d7b1-03125ea8a620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job ID: ftjob-MnBYCcyb27VCgEnJk6mhnrsl\n",
            "Status: validating_files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check job status**\n",
        "\n",
        "You can make a GET request to the https://api.openai.com/v1/alpha/fine-tunes endpoint to list your alpha fine-tune jobs. In this instance you'll want to check that the ID you got from the previous step ends up as status: succeeded."
      ],
      "metadata": {
        "id": "AqbRnQ5G_9yA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.FineTuningJob.retrieve(job_id)\n",
        "\n",
        "print(\"Job ID:\", response[\"id\"])\n",
        "print(\"Status:\", response[\"status\"])\n",
        "print(\"Trained Tokens:\", response[\"trained_tokens\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdz8RP9mACiI",
        "outputId": "d14232ef-bb29-426e-dc8d-3762c3db82d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job ID: ftjob-MnBYCcyb27VCgEnJk6mhnrsl\n",
            "Status: running\n",
            "Trained Tokens: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can track the progress of the fine-tune with the events endpoint. You can rerun the cell below a few times until the fine-tune is ready."
      ],
      "metadata": {
        "id": "v0wMJYPK_9Xg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "job_id=\"ftjob-MnBYCcyb27VCgEnJk6mhnrsl\"\n",
        "openai.api_key = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "YekcRTEffbIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.FineTuningJob.list_events(id=job_id, limit=50)\n",
        "\n",
        "events = response[\"data\"]\n",
        "events.reverse()\n",
        "\n",
        "for event in events:\n",
        "    print(event[\"message\"])"
      ],
      "metadata": {
        "id": "jLtetToBAK2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.FineTuningJob.retrieve(job_id)\n",
        "fine_tuned_model_id = response[\"fine_tuned_model\"]\n",
        "\n",
        "if fine_tuned_model_id is None:\n",
        "    raise RuntimeError(\"Fine-tuned model ID not found. Your job has likely not been completed yet.\")\n",
        "\n",
        "print(\"Fine-tuned model ID:\", fine_tuned_model_id)"
      ],
      "metadata": {
        "id": "qyk6mZkigA3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference**\n",
        "\n",
        "The last step is to use your fine-tuned model for inference. Similar to the classic FineTuning, you simply call ChatCompletions with your new fine-tuned model name filling the model parameter."
      ],
      "metadata": {
        "id": "72Fm6ztHssRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inference\n",
        "# testcase may be either recipe or review.\n",
        "test_df = recipe_df.loc[9000:]\n",
        "test_row = test_df.iloc[0]\n",
        "test_messages = []\n",
        "test_messages.append({\"role\": \"system\", \"content\": system_message_recipe})\n",
        "user_message = create_user_message_recipe(test_row)\n",
        "test_messages.append({\"role\": \"user\", \"content\": create_user_message_recipe(test_row)})\n",
        "\n",
        "pprint(test_messages)"
      ],
      "metadata": {
        "id": "tVSI1l03suSl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}